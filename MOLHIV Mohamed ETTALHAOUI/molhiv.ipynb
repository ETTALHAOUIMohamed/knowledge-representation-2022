{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB3HU__IKIcT",
        "outputId": "9ff6b820-9d25-4c59-f6d6-f7c6d1bd83b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chOKNVZRKJfT"
      },
      "outputs": [],
      "source": [
        "# import libraries \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0u3Z6TBdIxV",
        "outputId": "58aa64d2-a828-4162-ce09-611a9b061581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.8 MB 40 kB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.21.6)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit-pypi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDz-iCA7587l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONm-YWCVgWQT",
        "outputId": "80a173a3-240d-409f-b40c-fa970d2ab792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5pPYRVLewfp",
        "outputId": "43923b7a-ea22-420f-ed7f-803457640cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "dataset = PygGraphPropPredDataset(name = 'ogbg-molhiv')\n",
        "# dataset.data.to(device)\n",
        "split_idx = dataset.get_idx_split() \n",
        "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=100 ,shuffle=True)\n",
        "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=100, shuffle=False)\n",
        "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my9FyWOJZy_f",
        "outputId": "0c898ca4-aaf0-4c93-b934-09f8a8d59a95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PygGraphPropPredDataset(41127)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDH0Z6yJfanJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f302a3de-a241-47ad-fac3-f375c7d553bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ljF3EJEr93j"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve_hqFzvr9os"
      },
      "outputs": [],
      "source": [
        "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
        "atom_encoder = AtomEncoder(emb_dim = 18)\n",
        "bond_encoder = BondEncoder(emb_dim = 18)\n",
        "atom_encoder = atom_encoder.to(device)\n",
        "# bond_encoder = bond_encoder.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7LLNNpCSn4M",
        "outputId": "024ca7c3-6d99-4743-8bd8-2aa5b37ea9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): SAGEConv(18, 64, aggr=mean)\n",
            "  (conv2): SAGEConv(64, 128, aggr=mean)\n",
            "  (conv3): GATConv(128, 64, heads=1)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,SAGEConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels,emb_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(emb_dim, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels*2)\n",
        "        self.conv3 = GATConv(hidden_channels*2, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self,x,edge_index,batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x,batch)  \n",
        "\n",
        "        # x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model1 = GCN(hidden_channels=64,emb_dim = 18)\n",
        "print(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW3NJL0wqp_7"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model1 = model1.to(device)\n",
        "dataset.data = dataset.data.to(device)\n",
        "# optimizer = torch.optim.Adam(list(model.parameters())+list(atom_encoder.parameters())+list(bond_encoder.parameters()), \n",
        "#                           lr=0.01)\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def train():\n",
        "  model1.train()\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    data.x = atom_encoder(data.x)\n",
        "    # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "  # Iterate in batches over the training dataset.\n",
        "    out = model1(data.x, data.edge_index, data.batch) \n",
        "    loss = criterion(out, data.y.squeeze(1))  \n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "    optimizer.zero_grad()\n",
        "    return float(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaP0jdCbqqEz"
      },
      "outputs": [],
      "source": [
        "\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def test(test_loader):\n",
        "    model1.eval()\n",
        "    y_label = []\n",
        "    y_predi = []\n",
        "    for data in test_loader:\n",
        "      data = data.to(device)\n",
        "      data.x = atom_encoder(data.x)\n",
        "      # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "    # Iterate in batches over the training dataset.\n",
        "      out = model1(data.x,data.edge_index,data.batch)  \n",
        "      y_predi_loader = out[:,1]\n",
        "      y_label.append(data.y.view(y_predi_loader.shape).detach().cpu())\n",
        "      y_predi.append(y_predi_loader.detach().cpu())\n",
        "    \n",
        "    y_label = torch.cat(y_label, dim = 0).numpy()\n",
        "    y_predi = torch.cat(y_predi, dim = 0).numpy()\n",
        "    # # y_label = np.array(y_label)\n",
        "    # # y_predi = np.array(y_label)\n",
        "    # evaluator = Evaluator(name='ogbg-molhiv')\n",
        "    # y_label = torch.tensor(y_label)\n",
        "    # y_predi = torch.tensor(y_predi)\n",
        "\n",
        "    precision = evaluator.eval({'y_true': y_label.reshape(len(y_label),1),\n",
        "                                'y_pred': y_predi.reshape(len(y_label),1),\n",
        "                               })\n",
        "    \n",
        "    return precision['rocauc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESJNPgJkYPrR",
        "outputId": "5e31d0a8-18b2-4a3e-a7da-2ddcd110e244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss :0.9166, Train Acc: 0.5029, Test Acc: 0.6113\n",
            "Epoch: 002, Loss :0.0774, Train Acc: 0.5091, Test Acc: 0.6211\n",
            "Epoch: 003, Loss :0.1713, Train Acc: 0.5009, Test Acc: 0.6408\n",
            "Epoch: 004, Loss :0.5129, Train Acc: 0.4968, Test Acc: 0.6233\n",
            "Epoch: 005, Loss :0.3080, Train Acc: 0.4926, Test Acc: 0.5958\n",
            "Epoch: 006, Loss :0.1490, Train Acc: 0.4964, Test Acc: 0.5904\n",
            "Epoch: 007, Loss :0.0764, Train Acc: 0.4975, Test Acc: 0.5908\n",
            "Epoch: 008, Loss :0.2120, Train Acc: 0.4979, Test Acc: 0.5890\n",
            "Epoch: 009, Loss :0.1911, Train Acc: 0.4988, Test Acc: 0.5893\n",
            "Epoch: 010, Loss :0.1861, Train Acc: 0.5009, Test Acc: 0.5888\n",
            "Epoch: 011, Loss :0.2457, Train Acc: 0.5013, Test Acc: 0.5881\n",
            "Epoch: 012, Loss :0.2839, Train Acc: 0.5006, Test Acc: 0.5886\n",
            "Epoch: 013, Loss :0.2125, Train Acc: 0.5003, Test Acc: 0.5888\n",
            "Epoch: 014, Loss :0.2446, Train Acc: 0.5012, Test Acc: 0.5871\n",
            "Epoch: 015, Loss :0.1134, Train Acc: 0.5025, Test Acc: 0.5862\n",
            "Epoch: 016, Loss :0.1095, Train Acc: 0.5044, Test Acc: 0.5859\n",
            "Epoch: 017, Loss :0.1721, Train Acc: 0.5068, Test Acc: 0.5866\n",
            "Epoch: 018, Loss :0.1952, Train Acc: 0.5098, Test Acc: 0.5896\n",
            "Epoch: 019, Loss :0.1126, Train Acc: 0.5130, Test Acc: 0.5918\n",
            "Epoch: 020, Loss :0.1432, Train Acc: 0.5173, Test Acc: 0.5957\n",
            "Epoch: 021, Loss :0.2011, Train Acc: 0.5220, Test Acc: 0.5991\n",
            "Epoch: 022, Loss :0.1299, Train Acc: 0.5281, Test Acc: 0.6020\n",
            "Epoch: 023, Loss :0.1679, Train Acc: 0.5353, Test Acc: 0.6045\n",
            "Epoch: 024, Loss :0.2827, Train Acc: 0.5442, Test Acc: 0.5994\n",
            "Epoch: 025, Loss :0.1603, Train Acc: 0.5513, Test Acc: 0.5909\n",
            "Epoch: 026, Loss :0.0842, Train Acc: 0.5591, Test Acc: 0.5806\n",
            "Epoch: 027, Loss :0.1115, Train Acc: 0.5690, Test Acc: 0.5653\n",
            "Epoch: 028, Loss :0.1306, Train Acc: 0.5752, Test Acc: 0.5526\n",
            "Epoch: 029, Loss :0.1004, Train Acc: 0.5746, Test Acc: 0.5436\n",
            "Epoch: 030, Loss :0.0995, Train Acc: 0.5761, Test Acc: 0.5422\n",
            "Epoch: 031, Loss :0.1396, Train Acc: 0.5794, Test Acc: 0.5377\n",
            "Epoch: 032, Loss :0.2030, Train Acc: 0.5850, Test Acc: 0.5370\n",
            "Epoch: 033, Loss :0.2523, Train Acc: 0.5879, Test Acc: 0.5392\n",
            "Epoch: 034, Loss :0.0602, Train Acc: 0.5862, Test Acc: 0.5320\n",
            "Epoch: 035, Loss :0.1407, Train Acc: 0.5843, Test Acc: 0.5241\n",
            "Epoch: 036, Loss :0.1377, Train Acc: 0.5851, Test Acc: 0.5205\n",
            "Epoch: 037, Loss :0.0811, Train Acc: 0.5890, Test Acc: 0.5245\n",
            "Epoch: 038, Loss :0.1391, Train Acc: 0.5933, Test Acc: 0.5300\n",
            "Epoch: 039, Loss :0.0757, Train Acc: 0.5997, Test Acc: 0.5455\n",
            "Epoch: 040, Loss :0.0635, Train Acc: 0.6041, Test Acc: 0.5649\n",
            "Epoch: 041, Loss :0.2191, Train Acc: 0.6053, Test Acc: 0.5604\n",
            "Epoch: 042, Loss :0.4116, Train Acc: 0.5904, Test Acc: 0.5171\n",
            "Epoch: 043, Loss :0.3221, Train Acc: 0.5706, Test Acc: 0.4791\n",
            "Epoch: 044, Loss :0.1729, Train Acc: 0.5569, Test Acc: 0.4607\n",
            "Epoch: 045, Loss :0.1926, Train Acc: 0.5500, Test Acc: 0.4529\n",
            "Epoch: 046, Loss :0.2222, Train Acc: 0.5457, Test Acc: 0.4497\n",
            "Epoch: 047, Loss :0.1823, Train Acc: 0.5422, Test Acc: 0.4479\n",
            "Epoch: 048, Loss :0.1033, Train Acc: 0.5387, Test Acc: 0.4469\n",
            "Epoch: 049, Loss :0.1949, Train Acc: 0.5360, Test Acc: 0.4465\n",
            "Epoch: 050, Loss :0.0655, Train Acc: 0.5338, Test Acc: 0.4463\n",
            "Epoch: 051, Loss :0.1902, Train Acc: 0.5337, Test Acc: 0.4459\n",
            "Epoch: 052, Loss :0.2522, Train Acc: 0.5348, Test Acc: 0.4455\n",
            "Epoch: 053, Loss :0.1972, Train Acc: 0.5361, Test Acc: 0.4444\n",
            "Epoch: 054, Loss :0.3178, Train Acc: 0.5402, Test Acc: 0.4441\n",
            "Epoch: 055, Loss :0.2665, Train Acc: 0.5460, Test Acc: 0.4446\n",
            "Epoch: 056, Loss :0.1367, Train Acc: 0.5523, Test Acc: 0.4455\n",
            "Epoch: 057, Loss :0.2187, Train Acc: 0.5588, Test Acc: 0.4480\n",
            "Epoch: 058, Loss :0.2314, Train Acc: 0.5646, Test Acc: 0.4528\n",
            "Epoch: 059, Loss :0.2470, Train Acc: 0.5674, Test Acc: 0.4592\n",
            "Epoch: 060, Loss :0.2151, Train Acc: 0.5677, Test Acc: 0.4721\n",
            "Epoch: 061, Loss :0.2269, Train Acc: 0.5681, Test Acc: 0.4911\n",
            "Epoch: 062, Loss :0.2170, Train Acc: 0.5682, Test Acc: 0.5085\n",
            "Epoch: 063, Loss :0.2365, Train Acc: 0.5670, Test Acc: 0.5276\n",
            "Epoch: 064, Loss :0.1230, Train Acc: 0.5645, Test Acc: 0.5458\n",
            "Epoch: 065, Loss :0.1504, Train Acc: 0.5606, Test Acc: 0.5585\n",
            "Epoch: 066, Loss :0.0620, Train Acc: 0.5560, Test Acc: 0.5653\n",
            "Epoch: 067, Loss :0.1980, Train Acc: 0.5555, Test Acc: 0.5647\n",
            "Epoch: 068, Loss :0.0567, Train Acc: 0.5553, Test Acc: 0.5638\n",
            "Epoch: 069, Loss :0.1977, Train Acc: 0.5562, Test Acc: 0.5643\n",
            "Epoch: 070, Loss :0.4600, Train Acc: 0.5581, Test Acc: 0.5629\n",
            "Epoch: 071, Loss :0.0962, Train Acc: 0.5578, Test Acc: 0.5609\n",
            "Epoch: 072, Loss :0.1346, Train Acc: 0.5566, Test Acc: 0.5608\n",
            "Epoch: 073, Loss :0.1102, Train Acc: 0.5567, Test Acc: 0.5620\n",
            "Epoch: 074, Loss :0.2867, Train Acc: 0.5576, Test Acc: 0.5608\n",
            "Epoch: 075, Loss :0.1293, Train Acc: 0.5589, Test Acc: 0.5604\n",
            "Epoch: 076, Loss :0.2010, Train Acc: 0.5603, Test Acc: 0.5601\n",
            "Epoch: 077, Loss :0.1151, Train Acc: 0.5624, Test Acc: 0.5612\n",
            "Epoch: 078, Loss :0.1967, Train Acc: 0.5653, Test Acc: 0.5629\n",
            "Epoch: 079, Loss :0.3301, Train Acc: 0.5689, Test Acc: 0.5569\n",
            "Epoch: 080, Loss :0.2901, Train Acc: 0.5723, Test Acc: 0.5512\n",
            "Epoch: 081, Loss :0.2383, Train Acc: 0.5765, Test Acc: 0.5441\n",
            "Epoch: 082, Loss :0.2624, Train Acc: 0.5799, Test Acc: 0.5362\n",
            "Epoch: 083, Loss :0.1690, Train Acc: 0.5812, Test Acc: 0.5308\n",
            "Epoch: 084, Loss :0.1662, Train Acc: 0.5837, Test Acc: 0.5324\n",
            "Epoch: 085, Loss :0.1696, Train Acc: 0.5865, Test Acc: 0.5322\n",
            "Epoch: 086, Loss :0.1611, Train Acc: 0.5893, Test Acc: 0.5342\n",
            "Epoch: 087, Loss :0.2211, Train Acc: 0.5914, Test Acc: 0.5352\n",
            "Epoch: 088, Loss :0.0792, Train Acc: 0.5937, Test Acc: 0.5377\n",
            "Epoch: 089, Loss :0.1682, Train Acc: 0.5948, Test Acc: 0.5411\n",
            "Epoch: 090, Loss :0.1653, Train Acc: 0.5936, Test Acc: 0.5431\n",
            "Epoch: 091, Loss :0.1762, Train Acc: 0.5935, Test Acc: 0.5472\n",
            "Epoch: 092, Loss :0.1296, Train Acc: 0.5925, Test Acc: 0.5527\n",
            "Epoch: 093, Loss :0.2653, Train Acc: 0.5921, Test Acc: 0.5557\n",
            "Epoch: 094, Loss :0.2366, Train Acc: 0.5911, Test Acc: 0.5592\n",
            "Epoch: 095, Loss :0.0700, Train Acc: 0.5886, Test Acc: 0.5647\n",
            "Epoch: 096, Loss :0.1597, Train Acc: 0.5853, Test Acc: 0.5708\n",
            "Epoch: 097, Loss :0.0478, Train Acc: 0.5800, Test Acc: 0.5792\n",
            "Epoch: 098, Loss :0.2022, Train Acc: 0.5812, Test Acc: 0.5751\n",
            "Epoch: 099, Loss :0.1498, Train Acc: 0.5871, Test Acc: 0.5696\n",
            "Epoch: 100, Loss :0.2599, Train Acc: 0.5913, Test Acc: 0.5667\n",
            "Epoch: 101, Loss :0.1817, Train Acc: 0.5950, Test Acc: 0.5656\n",
            "Epoch: 102, Loss :0.1089, Train Acc: 0.5985, Test Acc: 0.5663\n",
            "Epoch: 103, Loss :0.2421, Train Acc: 0.6043, Test Acc: 0.5704\n",
            "Epoch: 104, Loss :0.1680, Train Acc: 0.6095, Test Acc: 0.5763\n",
            "Epoch: 105, Loss :0.1160, Train Acc: 0.6142, Test Acc: 0.5802\n",
            "Epoch: 106, Loss :0.2897, Train Acc: 0.6183, Test Acc: 0.5840\n",
            "Epoch: 107, Loss :0.1674, Train Acc: 0.6210, Test Acc: 0.5884\n",
            "Epoch: 108, Loss :0.1422, Train Acc: 0.6242, Test Acc: 0.5914\n",
            "Epoch: 109, Loss :0.2383, Train Acc: 0.6259, Test Acc: 0.5931\n",
            "Epoch: 110, Loss :0.1062, Train Acc: 0.6271, Test Acc: 0.5946\n",
            "Epoch: 111, Loss :0.0847, Train Acc: 0.6281, Test Acc: 0.5932\n",
            "Epoch: 112, Loss :0.2249, Train Acc: 0.6278, Test Acc: 0.5900\n",
            "Epoch: 113, Loss :0.1757, Train Acc: 0.6280, Test Acc: 0.5901\n",
            "Epoch: 114, Loss :0.1589, Train Acc: 0.6296, Test Acc: 0.5923\n",
            "Epoch: 115, Loss :0.1322, Train Acc: 0.6310, Test Acc: 0.5960\n",
            "Epoch: 116, Loss :0.0994, Train Acc: 0.6316, Test Acc: 0.5972\n",
            "Epoch: 117, Loss :0.2316, Train Acc: 0.6339, Test Acc: 0.5965\n",
            "Epoch: 118, Loss :0.1365, Train Acc: 0.6357, Test Acc: 0.5964\n",
            "Epoch: 119, Loss :0.3340, Train Acc: 0.6362, Test Acc: 0.5976\n",
            "Epoch: 120, Loss :0.1739, Train Acc: 0.6381, Test Acc: 0.6016\n",
            "Epoch: 121, Loss :0.1632, Train Acc: 0.6393, Test Acc: 0.6047\n",
            "Epoch: 122, Loss :0.0918, Train Acc: 0.6405, Test Acc: 0.6085\n",
            "Epoch: 123, Loss :0.0923, Train Acc: 0.6416, Test Acc: 0.6118\n",
            "Epoch: 124, Loss :0.1717, Train Acc: 0.6422, Test Acc: 0.6122\n",
            "Epoch: 125, Loss :0.1976, Train Acc: 0.6419, Test Acc: 0.6110\n",
            "Epoch: 126, Loss :0.1316, Train Acc: 0.6407, Test Acc: 0.6072\n",
            "Epoch: 127, Loss :0.2093, Train Acc: 0.6391, Test Acc: 0.6026\n",
            "Epoch: 128, Loss :0.1317, Train Acc: 0.6376, Test Acc: 0.5984\n",
            "Epoch: 129, Loss :0.2459, Train Acc: 0.6366, Test Acc: 0.5950\n",
            "Epoch: 130, Loss :0.1744, Train Acc: 0.6368, Test Acc: 0.5945\n",
            "Epoch: 131, Loss :0.2379, Train Acc: 0.6382, Test Acc: 0.5987\n",
            "Epoch: 132, Loss :0.1037, Train Acc: 0.6387, Test Acc: 0.6039\n",
            "Epoch: 133, Loss :0.1068, Train Acc: 0.6371, Test Acc: 0.6066\n",
            "Epoch: 134, Loss :0.2221, Train Acc: 0.6336, Test Acc: 0.6069\n",
            "Epoch: 135, Loss :0.1344, Train Acc: 0.6290, Test Acc: 0.6063\n",
            "Epoch: 136, Loss :0.1657, Train Acc: 0.6246, Test Acc: 0.6048\n",
            "Epoch: 137, Loss :0.1408, Train Acc: 0.6196, Test Acc: 0.6026\n",
            "Epoch: 138, Loss :0.2031, Train Acc: 0.6173, Test Acc: 0.6009\n",
            "Epoch: 139, Loss :0.1740, Train Acc: 0.6167, Test Acc: 0.6012\n",
            "Epoch: 140, Loss :0.2006, Train Acc: 0.6187, Test Acc: 0.6025\n",
            "Epoch: 141, Loss :0.2579, Train Acc: 0.6196, Test Acc: 0.6049\n",
            "Epoch: 142, Loss :0.1065, Train Acc: 0.6186, Test Acc: 0.6061\n",
            "Epoch: 143, Loss :0.1681, Train Acc: 0.6156, Test Acc: 0.6066\n",
            "Epoch: 144, Loss :0.1362, Train Acc: 0.6163, Test Acc: 0.6075\n",
            "Epoch: 145, Loss :0.0774, Train Acc: 0.6145, Test Acc: 0.6059\n",
            "Epoch: 146, Loss :0.1619, Train Acc: 0.6129, Test Acc: 0.6025\n",
            "Epoch: 147, Loss :0.0647, Train Acc: 0.6087, Test Acc: 0.5974\n",
            "Epoch: 148, Loss :0.1386, Train Acc: 0.6083, Test Acc: 0.5988\n",
            "Epoch: 149, Loss :0.2328, Train Acc: 0.6120, Test Acc: 0.6017\n",
            "Epoch: 150, Loss :0.1420, Train Acc: 0.6164, Test Acc: 0.6059\n",
            "Epoch: 151, Loss :0.1774, Train Acc: 0.6281, Test Acc: 0.6153\n",
            "Epoch: 152, Loss :0.1280, Train Acc: 0.6386, Test Acc: 0.6235\n",
            "Epoch: 153, Loss :0.2030, Train Acc: 0.6477, Test Acc: 0.6404\n",
            "Epoch: 154, Loss :0.1419, Train Acc: 0.6481, Test Acc: 0.6362\n",
            "Epoch: 155, Loss :0.1644, Train Acc: 0.6484, Test Acc: 0.6264\n",
            "Epoch: 156, Loss :0.2584, Train Acc: 0.6532, Test Acc: 0.6236\n",
            "Epoch: 157, Loss :0.1468, Train Acc: 0.6552, Test Acc: 0.6184\n",
            "Epoch: 158, Loss :0.1714, Train Acc: 0.6570, Test Acc: 0.6110\n",
            "Epoch: 159, Loss :0.1925, Train Acc: 0.6571, Test Acc: 0.6001\n",
            "Epoch: 160, Loss :0.1319, Train Acc: 0.6547, Test Acc: 0.5895\n",
            "Epoch: 161, Loss :0.1343, Train Acc: 0.6524, Test Acc: 0.5824\n",
            "Epoch: 162, Loss :0.1570, Train Acc: 0.6503, Test Acc: 0.5799\n",
            "Epoch: 163, Loss :0.2361, Train Acc: 0.6511, Test Acc: 0.5844\n",
            "Epoch: 164, Loss :0.1356, Train Acc: 0.6504, Test Acc: 0.5889\n",
            "Epoch: 165, Loss :0.1736, Train Acc: 0.6495, Test Acc: 0.5910\n",
            "Epoch: 166, Loss :0.1254, Train Acc: 0.6471, Test Acc: 0.5899\n",
            "Epoch: 167, Loss :0.2701, Train Acc: 0.6428, Test Acc: 0.5892\n",
            "Epoch: 168, Loss :0.1964, Train Acc: 0.6371, Test Acc: 0.5870\n",
            "Epoch: 169, Loss :0.2289, Train Acc: 0.6302, Test Acc: 0.5836\n",
            "Epoch: 170, Loss :0.2695, Train Acc: 0.6271, Test Acc: 0.5774\n",
            "Epoch: 171, Loss :0.2021, Train Acc: 0.6244, Test Acc: 0.5682\n",
            "Epoch: 172, Loss :0.2129, Train Acc: 0.6221, Test Acc: 0.5675\n",
            "Epoch: 173, Loss :0.1348, Train Acc: 0.6141, Test Acc: 0.5650\n",
            "Epoch: 174, Loss :0.2027, Train Acc: 0.6061, Test Acc: 0.5605\n",
            "Epoch: 175, Loss :0.1711, Train Acc: 0.6005, Test Acc: 0.5553\n",
            "Epoch: 176, Loss :0.1124, Train Acc: 0.5967, Test Acc: 0.5535\n",
            "Epoch: 177, Loss :0.1009, Train Acc: 0.5942, Test Acc: 0.5516\n",
            "Epoch: 178, Loss :0.2284, Train Acc: 0.5959, Test Acc: 0.5584\n",
            "Epoch: 179, Loss :0.2327, Train Acc: 0.5985, Test Acc: 0.5679\n",
            "Epoch: 180, Loss :0.1639, Train Acc: 0.5989, Test Acc: 0.5755\n",
            "Epoch: 181, Loss :0.1336, Train Acc: 0.5998, Test Acc: 0.5831\n",
            "Epoch: 182, Loss :0.3011, Train Acc: 0.6004, Test Acc: 0.5875\n",
            "Epoch: 183, Loss :0.1725, Train Acc: 0.6015, Test Acc: 0.5920\n",
            "Epoch: 184, Loss :0.1950, Train Acc: 0.6010, Test Acc: 0.5976\n",
            "Epoch: 185, Loss :0.1385, Train Acc: 0.5999, Test Acc: 0.6013\n",
            "Epoch: 186, Loss :0.0993, Train Acc: 0.5955, Test Acc: 0.6030\n",
            "Epoch: 187, Loss :0.2323, Train Acc: 0.5922, Test Acc: 0.6056\n",
            "Epoch: 188, Loss :0.1966, Train Acc: 0.5906, Test Acc: 0.6085\n",
            "Epoch: 189, Loss :0.1674, Train Acc: 0.5898, Test Acc: 0.6116\n",
            "Epoch: 190, Loss :0.1065, Train Acc: 0.5864, Test Acc: 0.6129\n",
            "Epoch: 191, Loss :0.1620, Train Acc: 0.5839, Test Acc: 0.6138\n",
            "Epoch: 192, Loss :0.1306, Train Acc: 0.5815, Test Acc: 0.6136\n",
            "Epoch: 193, Loss :0.1739, Train Acc: 0.5810, Test Acc: 0.6131\n",
            "Epoch: 194, Loss :0.0604, Train Acc: 0.5800, Test Acc: 0.6133\n",
            "Epoch: 195, Loss :0.2465, Train Acc: 0.5809, Test Acc: 0.6123\n",
            "Epoch: 196, Loss :0.1975, Train Acc: 0.5849, Test Acc: 0.6121\n",
            "Epoch: 197, Loss :0.2205, Train Acc: 0.5911, Test Acc: 0.6159\n",
            "Epoch: 198, Loss :0.1704, Train Acc: 0.5961, Test Acc: 0.6178\n",
            "Epoch: 199, Loss :0.1031, Train Acc: 0.5985, Test Acc: 0.6185\n"
          ]
        }
      ],
      "source": [
        "losses1 = []\n",
        "train_acc_list1 = []\n",
        "test_acc_list1 = []\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    loss = train()\n",
        "    losses1.append(loss)\n",
        "    train_acc = test(train_loader)\n",
        "    train_acc_list1.append(train_acc)\n",
        "    test_acc = test(test_loader)\n",
        "    test_acc_list1.append(test_acc)\n",
        "    # if epoch % 10==0:\n",
        "    print(f'Epoch: {epoch:03d}, Loss :{loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "vRu63kzI_E3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(list(zip(train_acc_list1,test_acc_list1, losses1)), columns =['train_acc1', 'test_acc1','loss1'])\n",
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nvehTL53_pgz",
        "outputId": "dbcbe2c6-4370-49a1-b8f5-c0006787d82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_acc1  test_acc1     loss1\n",
              "0    0.502927   0.611337  0.916597\n",
              "1    0.509117   0.621124  0.077439\n",
              "2    0.500871   0.640795  0.171282\n",
              "3    0.496803   0.623273  0.512851\n",
              "4    0.492562   0.595774  0.308031"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0687b3e0-8a1a-4ea2-97e5-27fa6a94b877\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_acc1</th>\n",
              "      <th>test_acc1</th>\n",
              "      <th>loss1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.502927</td>\n",
              "      <td>0.611337</td>\n",
              "      <td>0.916597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.509117</td>\n",
              "      <td>0.621124</td>\n",
              "      <td>0.077439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.500871</td>\n",
              "      <td>0.640795</td>\n",
              "      <td>0.171282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.496803</td>\n",
              "      <td>0.623273</td>\n",
              "      <td>0.512851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.492562</td>\n",
              "      <td>0.595774</td>\n",
              "      <td>0.308031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0687b3e0-8a1a-4ea2-97e5-27fa6a94b877')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0687b3e0-8a1a-4ea2-97e5-27fa6a94b877 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0687b3e0-8a1a-4ea2-97e5-27fa6a94b877');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_float = [loss for loss in losses1] \n",
        "loss_indices = [i for i,l in enumerate(losses_float)] \n",
        "fig = px.line(df1, x=loss_indices, y=[\"train_acc1\", \"test_acc1\", \"loss1\"], title=\"Mesure de performance pour le modele 1: 2 couches SAGE et une GAT\",\n",
        "            labels={\"train_acc1\": \"train_acc1\", \"loss\": \"loss1\",\"test_acc1\":\"test_acc1\"})\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "B_4AjWUt_E0G",
        "outputId": "cf986d4b-1d5d-48bb-8498-17a92954786b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"c28b3e39-6e95-4b07-9316-74832350d5e8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c28b3e39-6e95-4b07-9316-74832350d5e8\")) {                    Plotly.newPlot(                        \"c28b3e39-6e95-4b07-9316-74832350d5e8\",                        [{\"hovertemplate\":\"variable=train_acc1<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"train_acc1\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train_acc1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[0.5029272065598994,0.5091169034161392,0.5008706125413316,0.4968034822861309,0.49256212956420564,0.4964232813193942,0.4974648997155234,0.49790862556402204,0.4988492090261565,0.5009184388190672,0.5012773409450759,0.5005959702695865,0.5002509726214296,0.5012006933118667,0.502530820012032,0.504412268870414,0.5067785290666894,0.509779192278245,0.5130487181122265,0.5172530733893976,0.5219952820632902,0.5280806248521127,0.5353274849262645,0.5442220064030826,0.5513414066277277,0.559131925378294,0.568967235360238,0.5752060143825356,0.5746392371088446,0.5760749993951232,0.5793514197996894,0.5850043141045383,0.5879281759006411,0.5861880657392436,0.5842634553311793,0.5850888943384759,0.5890352286413892,0.5933372612735712,0.599661415071398,0.6040941497953877,0.605332647908787,0.5903795545687064,0.5706358880391451,0.5569374399480339,0.5500126511525671,0.5457169748531175,0.5422438028831504,0.5386697497614326,0.5360268481242462,0.5338410001299974,0.5337339933188792,0.5347906567444995,0.5361104287736009,0.5402174142602479,0.5459658970446334,0.5523297138461022,0.5588078421152565,0.5645692041625368,0.5674249788703197,0.5676749775375403,0.5680708258475554,0.5682231215293911,0.5670311681750313,0.5645084217307843,0.5606279446736597,0.5559754935692367,0.5554774056976527,0.5552678389452916,0.5562425620654883,0.5581351601365259,0.5577914440070649,0.5565567648193797,0.5566863263595478,0.5576163244772532,0.5589318418642837,0.5603322854952998,0.562374885329707,0.5652637488502215,0.5688967518319566,0.5722995812406987,0.5764653115443715,0.5798720367699496,0.5811602321783809,0.5836614362933477,0.586536741346058,0.5893287989442746,0.5913896347897263,0.5936849372958027,0.5947650525135605,0.5935587333346183,0.5934961952222523,0.5924877937907241,0.592081257614784,0.5910638855523838,0.5885713316886152,0.5853019724520641,0.5799842209165995,0.5811627695853989,0.5870930486120025,0.5912593940446493,0.5949931243958921,0.598492746911745,0.6043195048580836,0.6094824489350683,0.6141979251289618,0.618318712571965,0.621021986554921,0.6242039590315902,0.6258751747478895,0.6271405847539053,0.6281270465853575,0.6277741804124071,0.6280355333352744,0.6295798915158541,0.630983282639871,0.6316110986490537,0.6339359657914475,0.635667758896508,0.6362366250456734,0.6381306712328374,0.6393283017150205,0.6404562303953271,0.641588426532891,0.6421559599026128,0.6418897500238875,0.6406863655227593,0.6391094055065527,0.6376155776081571,0.6365504048983951,0.6367533590142844,0.6381554301740446,0.6386604638769611,0.6370925001220006,0.6336486749301726,0.6289974284533238,0.6246087395269166,0.6196172626514601,0.6173487182557567,0.6166829692931717,0.6186568028343503,0.6195740370258432,0.6186223556118011,0.6156307527374265,0.6162584918554874,0.6145436532427754,0.6128946077999173,0.6087336319306068,0.6083348233124038,0.6120305310039356,0.6164343802965168,0.6281386186991826,0.6386354358168277,0.6477098825185678,0.648074718076139,0.6483577158497822,0.6531905791562316,0.6551703204985989,0.6570456436976141,0.6571017870316869,0.6547278761687964,0.6523631409797692,0.6503083410873758,0.651058772805394,0.6504264330352144,0.6494987160207881,0.6471058514963833,0.6427992951032042,0.6370878482091341,0.6301851271656129,0.627073984227273,0.624400338956569,0.6220961427107421,0.6140969158253411,0.606138338712978,0.6004841628894331,0.5967499968218336,0.5941506668203123,0.5959454209389082,0.5984770226773448,0.5988645283006488,0.5998010749788908,0.6004042730139227,0.6014855672288809,0.6010024603108534,0.5998789016093005,0.5955233399411854,0.5921966327429872,0.5906096896961386,0.5897839815699157,0.5864347068274804,0.5839294915589952,0.5815385621278213,0.58095576587043,0.5800304324807782,0.5809349283764327,0.5849195544579832,0.5910525953726717,0.5960806596068997,0.5984577358209696],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=test_acc1<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"test_acc1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test_acc1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[0.611336642268101,0.6211243940593678,0.6407945306012091,0.6232729484926322,0.5957743486741729,0.5903822012785107,0.5908012900983024,0.588953050464474,0.5892697811854226,0.5888101353830704,0.5880801096969814,0.5886034879005002,0.588815929237722,0.5870681164178528,0.5862106259294309,0.5858552695108055,0.5865968829061976,0.5895594739179976,0.5918036269530118,0.5956584715811429,0.599130921802275,0.6019892234303482,0.6045481759014273,0.5994041986133375,0.5908930261302845,0.5806282469727109,0.5652803260008884,0.5526043376658492,0.5435968249676509,0.5422043685664073,0.5377469630545202,0.5369686552463353,0.5391722512987891,0.531982077676278,0.5241458892601248,0.5205392147395662,0.5245446995886363,0.5299590567604627,0.5454730682322949,0.5648892408119122,0.5604250758029317,0.5171285656347167,0.47908032213831864,0.4607447034512061,0.45294907201761336,0.4497035477703316,0.4479113153981344,0.4468954595492381,0.44645126402595636,0.4462581355375732,0.44592402325267,0.4455107282875297,0.44440506769153515,0.4441443442322177,0.4445605361246837,0.4454749995171787,0.4479595975202302,0.4528109851484193,0.4592479576662354,0.4720678267251202,0.49113347109832173,0.508464821645841,0.5276038548446281,0.5457869020259178,0.5585488325382877,0.5653276424805425,0.5646980436084127,0.5637806832885919,0.564346549759555,0.56292319280017,0.5608760308233067,0.5607804322215569,0.5619672067826725,0.5607833291488826,0.5604299040151413,0.560144073852334,0.5612178682477452,0.5628922922420285,0.556887927538191,0.5511674617122773,0.5440641959095387,0.5362173854265242,0.5307982000424882,0.5324388265513047,0.5321887251588483,0.5342194712141989,0.5352015295776279,0.5376581239498638,0.5411344367407636,0.5431313853106472,0.5472044651306515,0.5527221460437629,0.5556866683404469,0.5591880878348365,0.5646661774078294,0.5707825566349292,0.5791952335889067,0.5751424322601828,0.5695996446435814,0.566733617875973,0.5656134726433495,0.5662720407887367,0.5703586396029279,0.5762558179957125,0.5801772919523359,0.5840118580891869,0.5884045655574655,0.5913546032175206,0.5931323509530891,0.5946416500898046,0.5931825643600688,0.5900384325691883,0.5901098901098901,0.5923327990111822,0.5960196218544198,0.5972305374765833,0.5965198246393325,0.5964010506189769,0.59759168774986,0.6016473860059097,0.6046775719886441,0.6085179319801465,0.6117750439367311,0.61215743834373,0.6110363274686649,0.6072481121690261,0.6026246161571294,0.5983989648313023,0.5950433573456421,0.5944900442264239,0.5986934857760868,0.6038712605496437,0.6065682998899167,0.6068599239073755,0.6063249579945537,0.6048031055060932,0.602641032078642,0.6008980474709824,0.6012089843372797,0.6025183954885185,0.6049151200293554,0.6060854786689585,0.6065528496108462,0.607492419706831,0.6059271133084841,0.6025232237007281,0.5973695899882192,0.5987620463894628,0.6017439502501014,0.6059213194538328,0.6153257111956584,0.6235249811699723,0.6403638540721142,0.6361797253712895,0.6264392900596767,0.6235945074257904,0.6183829351667663,0.6110102551227331,0.6000714575407018,0.5894841538075282,0.5823615751559512,0.5798586299465034,0.5844348094787462,0.5889491878947064,0.5910369068541301,0.589914830336623,0.5891616292319279,0.587043009714363,0.5835647656385794,0.5773759632283358,0.5682207072365245,0.5674858533382259,0.5649915989107553,0.5604743235674694,0.5552608200235617,0.5535139728461346,0.5516348326541649,0.5584097800266516,0.5679213580795304,0.5754997199636918,0.5831244326850654,0.5875074837289249,0.5920334498541879,0.5975627184766025,0.6013103767936809,0.6029992854245929,0.6055814133142781,0.6085324166167752,0.6115935031576508,0.6128797388902837,0.6137719925066146,0.6136232835705595,0.6131365997798336,0.6132659958670504,0.6122849031460631,0.6121149500762857,0.6159485505706946,0.6178296220475482,0.6184843276231676],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=loss1<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"loss1\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"loss1\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[0.916596531867981,0.07743909955024719,0.1712816208600998,0.5128512978553772,0.30803051590919495,0.14899441599845886,0.07637481391429901,0.2120341807603836,0.19107681512832642,0.1861436814069748,0.24574799835681915,0.2839025855064392,0.21246643364429474,0.2445663958787918,0.11339236050844193,0.10949191451072693,0.17205047607421875,0.1951638162136078,0.1126052662730217,0.14324204623699188,0.2011110484600067,0.12987549602985382,0.16790109872817993,0.28265467286109924,0.16031788289546967,0.08424662798643112,0.11149215698242188,0.13058766722679138,0.10041537135839462,0.0995386615395546,0.13958346843719482,0.20303064584732056,0.25228559970855713,0.060161832720041275,0.14073830842971802,0.13765595853328705,0.08111197501420975,0.13905058801174164,0.07571455836296082,0.06347495317459106,0.21912258863449097,0.41161608695983887,0.32210785150527954,0.17293491959571838,0.19259139895439148,0.22222395241260529,0.1822933554649353,0.10332686454057693,0.19486939907073975,0.06547034531831741,0.19017621874809265,0.2522214949131012,0.19719751179218292,0.3178302049636841,0.26645421981811523,0.1367216855287552,0.2187395840883255,0.23142187297344208,0.24698269367218018,0.21512936055660248,0.22692237794399261,0.21704190969467163,0.23646818101406097,0.12295638024806976,0.15039673447608948,0.06198233738541603,0.19799430668354034,0.056658461689949036,0.19773195683956146,0.4599922299385071,0.09619886428117752,0.13461703062057495,0.11016156524419785,0.28669270873069763,0.12934018671512604,0.2010115385055542,0.11505607515573502,0.19666342437267303,0.3301370143890381,0.2900520861148834,0.2383047491312027,0.26243090629577637,0.1689736247062683,0.16617654263973236,0.16956865787506104,0.16106075048446655,0.22114162147045135,0.07916410267353058,0.16821126639842987,0.16528236865997314,0.17623712122440338,0.1296369582414627,0.26525670289993286,0.23663252592086792,0.06999877095222473,0.15973414480686188,0.04775804653763771,0.20223814249038696,0.1498316377401352,0.2598823606967926,0.18169187009334564,0.10885681211948395,0.2420755922794342,0.167984277009964,0.1159728467464447,0.2897103428840637,0.16744564473628998,0.1421855092048645,0.23833249509334564,0.1061788946390152,0.08470068126916885,0.22492608428001404,0.17565548419952393,0.15892736613750458,0.1321927309036255,0.09942661225795746,0.2316151112318039,0.13650237023830414,0.3340170383453369,0.17388570308685303,0.163238063454628,0.09183207154273987,0.09233491122722626,0.17165279388427734,0.19756978750228882,0.1315775215625763,0.20931625366210938,0.13170881569385529,0.24585121870040894,0.17439395189285278,0.23794008791446686,0.10367924720048904,0.10679392516613007,0.22213828563690186,0.13436275720596313,0.16566792130470276,0.1408160924911499,0.20309968292713165,0.17403745651245117,0.2006392478942871,0.25793445110321045,0.1064906045794487,0.16810405254364014,0.13624867796897888,0.07735404372215271,0.16190598905086517,0.06474854797124863,0.1386457085609436,0.23283690214157104,0.14202263951301575,0.17741695046424866,0.1279737502336502,0.20303960144519806,0.14194121956825256,0.1644454002380371,0.2584233582019806,0.14684848487377167,0.17138466238975525,0.19247692823410034,0.13192278146743774,0.1342860758304596,0.1570139080286026,0.23609139025211334,0.1355592906475067,0.17363481223583221,0.12536197900772095,0.2701343894004822,0.1963934451341629,0.22893600165843964,0.26954376697540283,0.20211297273635864,0.21290422976016998,0.13475774228572845,0.20268219709396362,0.17107202112674713,0.11242789030075073,0.10087698698043823,0.22837643325328827,0.2327142357826233,0.16387522220611572,0.13358263671398163,0.30107924342155457,0.1724846065044403,0.1949959397315979,0.13850639760494232,0.0992693230509758,0.23226624727249146,0.19655828177928925,0.16735397279262543,0.10645267367362976,0.16197361052036285,0.13059140741825104,0.17392830550670624,0.060445260256528854,0.24654081463813782,0.19747595489025116,0.22046466171741486,0.17044654488563538,0.10309679061174393],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Mesure de performance pour le modele 1: 2 couches SAGE et une GAT\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c28b3e39-6e95-4b07-9316-74832350d5e8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**model 2**"
      ],
      "metadata": {
        "id": "egR2fbTnG8G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "set batch_size  = 32"
      ],
      "metadata": {
        "id": "jwYFr77UB2G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "dataset = PygGraphPropPredDataset(name = 'ogbg-molhiv')\n",
        "# dataset.data.to(device) \n",
        "split_idx = dataset.get_idx_split() \n",
        "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=64 ,shuffle=True)\n",
        "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn2RE9vwyaav",
        "outputId": "e4ef5bbc-12b9-44db-8a99-e45d1c00d3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning:\n",
            "\n",
            "'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
        "atom_encoder = AtomEncoder(emb_dim = 24)\n",
        "bond_encoder = BondEncoder(emb_dim = 24)\n",
        "atom_encoder = atom_encoder.to(device)\n",
        "# bond_encoder = bond_encoder.to(device)"
      ],
      "metadata": {
        "id": "w57AD2TOGsoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,SAGEConv,GATConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(24, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels,aggr=\"add\")\n",
        "        # self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        # self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = SAGEConv(hidden_channels, hidden_channels, aggr=\"add\")\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self,x,edge_index,batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x,batch) \n",
        "\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model2 = GCN(hidden_channels=64)\n",
        "print(model2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw6GUHAxG7CQ",
        "outputId": "fc36db2d-e4a3-4afa-ea1e-7f95eee55bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GATConv(24, 64, heads=1)\n",
            "  (conv2): SAGEConv(64, 64, aggr=add)\n",
            "  (conv3): SAGEConv(64, 64, aggr=add)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model2 = model2.to(device)\n",
        "dataset.data = dataset.data.to(device)\n",
        "# optimizer = torch.optim.Adam(list(model.parameters())+list(atom_encoder.parameters())+list(bond_encoder.parameters()), \n",
        "#                           lr=0.01)\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def train():\n",
        "  model2.train()\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    data.x = atom_encoder(data.x)\n",
        "    # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "  # Iterate in batches over the training dataset.\n",
        "    optimizer.zero_grad()\n",
        "    out = model2(data.x, data.edge_index, data.batch)  \n",
        "    loss = criterion(out, data.y.squeeze(1))  \n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "    return float(loss)"
      ],
      "metadata": {
        "id": "g75jBGMxG66X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def test(test_loader):\n",
        "    model2.eval()\n",
        "    y_label = []\n",
        "    y_predi = []\n",
        "    for data in test_loader:\n",
        "      data = data.to(device)\n",
        "      data.x = atom_encoder(data.x)\n",
        "      # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "    # Iterate in batches over the training dataset.\n",
        "      out = model2(data.x,data.edge_index,data.batch)  \n",
        "      y_predi_loader = out[:,1]\n",
        "      y_label.append(data.y.view(y_predi_loader.shape).detach().cpu())\n",
        "      y_predi.append(y_predi_loader.detach().cpu())\n",
        "    \n",
        "    y_label = torch.cat(y_label, dim = 0).numpy()\n",
        "    y_predi = torch.cat(y_predi, dim = 0).numpy()\n",
        "    # # y_label = np.array(y_label)\n",
        "    # # y_predi = np.array(y_label)\n",
        "    # evaluator = Evaluator(name='ogbg-molhiv')\n",
        "    # y_label = torch.tensor(y_label)\n",
        "    # y_predi = torch.tensor(y_predi)\n",
        "\n",
        "    precision = evaluator.eval({'y_true': y_label.reshape(len(y_label),1),\n",
        "                                'y_pred': y_predi.reshape(len(y_label),1),\n",
        "                               })\n",
        "    \n",
        "    return precision['rocauc']"
      ],
      "metadata": {
        "id": "UMNvaNqQG63C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses2 = []\n",
        "train_acc_list2 = []\n",
        "test_acc_list2 = []\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "    loss = train()\n",
        "    losses2.append(loss)\n",
        "    train_acc = test(train_loader)\n",
        "    train_acc_list2.append(train_acc)\n",
        "    test_acc = test(test_loader)\n",
        "    test_acc_list2.append(test_acc)\n",
        "    # if epoch % 10==0:\n",
        "    print(f'Epoch: {epoch:03d}, loss:{loss:.4f} Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCMU0n53G60S",
        "outputId": "f3904fc1-d186-463c-f831-5d1b68c27663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, loss:0.5792 Train Acc: 0.5042, Test Acc: 0.5842\n",
            "Epoch: 002, loss:0.1438 Train Acc: 0.5120, Test Acc: 0.5455\n",
            "Epoch: 003, loss:0.5093 Train Acc: 0.5149, Test Acc: 0.5486\n",
            "Epoch: 004, loss:0.1308 Train Acc: 0.5151, Test Acc: 0.5507\n",
            "Epoch: 005, loss:0.2634 Train Acc: 0.5184, Test Acc: 0.5460\n",
            "Epoch: 006, loss:0.2296 Train Acc: 0.5225, Test Acc: 0.5738\n",
            "Epoch: 007, loss:0.1528 Train Acc: 0.5095, Test Acc: 0.5850\n",
            "Epoch: 008, loss:0.2360 Train Acc: 0.4901, Test Acc: 0.5752\n",
            "Epoch: 009, loss:0.2752 Train Acc: 0.4660, Test Acc: 0.5422\n",
            "Epoch: 010, loss:0.1417 Train Acc: 0.4459, Test Acc: 0.4950\n",
            "Epoch: 011, loss:0.1327 Train Acc: 0.4325, Test Acc: 0.4608\n",
            "Epoch: 012, loss:0.3359 Train Acc: 0.4304, Test Acc: 0.4612\n",
            "Epoch: 013, loss:0.0900 Train Acc: 0.4309, Test Acc: 0.4670\n",
            "Epoch: 014, loss:0.1808 Train Acc: 0.4309, Test Acc: 0.4745\n",
            "Epoch: 015, loss:0.2198 Train Acc: 0.4331, Test Acc: 0.4873\n",
            "Epoch: 016, loss:0.1427 Train Acc: 0.4371, Test Acc: 0.5004\n",
            "Epoch: 017, loss:0.2472 Train Acc: 0.4474, Test Acc: 0.5217\n",
            "Epoch: 018, loss:0.1463 Train Acc: 0.4651, Test Acc: 0.5406\n",
            "Epoch: 019, loss:0.1003 Train Acc: 0.4780, Test Acc: 0.5490\n",
            "Epoch: 020, loss:0.1722 Train Acc: 0.4862, Test Acc: 0.5506\n",
            "Epoch: 021, loss:0.3165 Train Acc: 0.4967, Test Acc: 0.5518\n",
            "Epoch: 022, loss:0.1532 Train Acc: 0.5054, Test Acc: 0.5486\n",
            "Epoch: 023, loss:0.2762 Train Acc: 0.5112, Test Acc: 0.5467\n",
            "Epoch: 024, loss:0.1512 Train Acc: 0.5169, Test Acc: 0.5401\n",
            "Epoch: 025, loss:0.0583 Train Acc: 0.5199, Test Acc: 0.5289\n",
            "Epoch: 026, loss:0.0893 Train Acc: 0.5221, Test Acc: 0.5163\n",
            "Epoch: 027, loss:0.3910 Train Acc: 0.5240, Test Acc: 0.5185\n",
            "Epoch: 028, loss:0.1971 Train Acc: 0.5260, Test Acc: 0.5248\n",
            "Epoch: 029, loss:0.2697 Train Acc: 0.5296, Test Acc: 0.5321\n",
            "Epoch: 030, loss:0.2429 Train Acc: 0.5331, Test Acc: 0.5389\n",
            "Epoch: 031, loss:0.2158 Train Acc: 0.5377, Test Acc: 0.5541\n",
            "Epoch: 032, loss:0.1210 Train Acc: 0.5430, Test Acc: 0.5699\n",
            "Epoch: 033, loss:0.2114 Train Acc: 0.5488, Test Acc: 0.5810\n",
            "Epoch: 034, loss:0.2258 Train Acc: 0.5542, Test Acc: 0.5903\n",
            "Epoch: 035, loss:0.1930 Train Acc: 0.5557, Test Acc: 0.5930\n",
            "Epoch: 036, loss:0.1845 Train Acc: 0.5528, Test Acc: 0.5862\n",
            "Epoch: 037, loss:0.1646 Train Acc: 0.5404, Test Acc: 0.5714\n",
            "Epoch: 038, loss:0.2460 Train Acc: 0.5319, Test Acc: 0.5645\n",
            "Epoch: 039, loss:0.0732 Train Acc: 0.5203, Test Acc: 0.5562\n",
            "Epoch: 040, loss:0.0734 Train Acc: 0.5090, Test Acc: 0.5469\n",
            "Epoch: 041, loss:0.0910 Train Acc: 0.5040, Test Acc: 0.5409\n",
            "Epoch: 042, loss:0.3372 Train Acc: 0.5086, Test Acc: 0.5485\n",
            "Epoch: 043, loss:0.2108 Train Acc: 0.5186, Test Acc: 0.5607\n",
            "Epoch: 044, loss:0.2665 Train Acc: 0.5322, Test Acc: 0.5757\n",
            "Epoch: 045, loss:0.2373 Train Acc: 0.5424, Test Acc: 0.5864\n",
            "Epoch: 046, loss:0.1117 Train Acc: 0.5443, Test Acc: 0.5896\n",
            "Epoch: 047, loss:0.1799 Train Acc: 0.5492, Test Acc: 0.5951\n",
            "Epoch: 048, loss:0.1088 Train Acc: 0.5544, Test Acc: 0.6015\n",
            "Epoch: 049, loss:0.1715 Train Acc: 0.5604, Test Acc: 0.6088\n",
            "Epoch: 050, loss:0.3147 Train Acc: 0.5693, Test Acc: 0.6206\n",
            "Epoch: 051, loss:0.0624 Train Acc: 0.5770, Test Acc: 0.6277\n",
            "Epoch: 052, loss:0.1111 Train Acc: 0.5810, Test Acc: 0.6262\n",
            "Epoch: 053, loss:0.0256 Train Acc: 0.5819, Test Acc: 0.6118\n",
            "Epoch: 054, loss:0.2116 Train Acc: 0.5776, Test Acc: 0.6079\n",
            "Epoch: 055, loss:0.0847 Train Acc: 0.5744, Test Acc: 0.6101\n",
            "Epoch: 056, loss:0.1822 Train Acc: 0.5737, Test Acc: 0.6093\n",
            "Epoch: 057, loss:0.2636 Train Acc: 0.5757, Test Acc: 0.6134\n",
            "Epoch: 058, loss:0.0154 Train Acc: 0.5767, Test Acc: 0.6149\n",
            "Epoch: 059, loss:0.1679 Train Acc: 0.5776, Test Acc: 0.6256\n",
            "Epoch: 060, loss:0.2663 Train Acc: 0.5761, Test Acc: 0.6366\n",
            "Epoch: 061, loss:0.2582 Train Acc: 0.5723, Test Acc: 0.6392\n",
            "Epoch: 062, loss:0.0888 Train Acc: 0.5684, Test Acc: 0.6372\n",
            "Epoch: 063, loss:0.0713 Train Acc: 0.5646, Test Acc: 0.6338\n",
            "Epoch: 064, loss:0.2378 Train Acc: 0.5583, Test Acc: 0.6249\n",
            "Epoch: 065, loss:0.1568 Train Acc: 0.5541, Test Acc: 0.6179\n",
            "Epoch: 066, loss:0.1498 Train Acc: 0.5523, Test Acc: 0.6166\n",
            "Epoch: 067, loss:0.2873 Train Acc: 0.5579, Test Acc: 0.6208\n",
            "Epoch: 068, loss:0.2148 Train Acc: 0.5650, Test Acc: 0.6262\n",
            "Epoch: 069, loss:0.2807 Train Acc: 0.5727, Test Acc: 0.6300\n",
            "Epoch: 070, loss:0.0467 Train Acc: 0.5816, Test Acc: 0.6295\n",
            "Epoch: 071, loss:0.2077 Train Acc: 0.5935, Test Acc: 0.6129\n",
            "Epoch: 072, loss:0.1946 Train Acc: 0.5951, Test Acc: 0.6010\n",
            "Epoch: 073, loss:0.2272 Train Acc: 0.5931, Test Acc: 0.6163\n",
            "Epoch: 074, loss:0.1661 Train Acc: 0.5886, Test Acc: 0.6272\n",
            "Epoch: 075, loss:0.2448 Train Acc: 0.5845, Test Acc: 0.6329\n",
            "Epoch: 076, loss:0.1199 Train Acc: 0.5823, Test Acc: 0.6350\n",
            "Epoch: 077, loss:0.1177 Train Acc: 0.5807, Test Acc: 0.6364\n",
            "Epoch: 078, loss:0.0958 Train Acc: 0.5804, Test Acc: 0.6371\n",
            "Epoch: 079, loss:0.2125 Train Acc: 0.5827, Test Acc: 0.6381\n",
            "Epoch: 080, loss:0.2421 Train Acc: 0.5853, Test Acc: 0.6395\n",
            "Epoch: 081, loss:0.0389 Train Acc: 0.5881, Test Acc: 0.6390\n",
            "Epoch: 082, loss:0.0893 Train Acc: 0.5906, Test Acc: 0.6364\n",
            "Epoch: 083, loss:0.0954 Train Acc: 0.5924, Test Acc: 0.6330\n",
            "Epoch: 084, loss:0.1417 Train Acc: 0.5927, Test Acc: 0.6322\n",
            "Epoch: 085, loss:0.2830 Train Acc: 0.5962, Test Acc: 0.6339\n",
            "Epoch: 086, loss:0.0702 Train Acc: 0.5987, Test Acc: 0.6310\n",
            "Epoch: 087, loss:0.1327 Train Acc: 0.5987, Test Acc: 0.6291\n",
            "Epoch: 088, loss:0.2102 Train Acc: 0.5979, Test Acc: 0.6258\n",
            "Epoch: 089, loss:0.0746 Train Acc: 0.5968, Test Acc: 0.6186\n",
            "Epoch: 090, loss:0.2370 Train Acc: 0.5959, Test Acc: 0.6213\n",
            "Epoch: 091, loss:0.0425 Train Acc: 0.5951, Test Acc: 0.6197\n",
            "Epoch: 092, loss:0.0978 Train Acc: 0.5938, Test Acc: 0.6171\n",
            "Epoch: 093, loss:0.0947 Train Acc: 0.5925, Test Acc: 0.6111\n",
            "Epoch: 094, loss:0.2073 Train Acc: 0.5920, Test Acc: 0.6052\n",
            "Epoch: 095, loss:0.0849 Train Acc: 0.5917, Test Acc: 0.5961\n",
            "Epoch: 096, loss:0.2550 Train Acc: 0.5925, Test Acc: 0.5944\n",
            "Epoch: 097, loss:0.1473 Train Acc: 0.5949, Test Acc: 0.6020\n",
            "Epoch: 098, loss:0.0978 Train Acc: 0.5960, Test Acc: 0.6007\n",
            "Epoch: 099, loss:0.0920 Train Acc: 0.5960, Test Acc: 0.5907\n",
            "Epoch: 100, loss:0.1292 Train Acc: 0.5957, Test Acc: 0.5776\n",
            "Epoch: 101, loss:0.0336 Train Acc: 0.5936, Test Acc: 0.5617\n",
            "Epoch: 102, loss:0.1563 Train Acc: 0.5953, Test Acc: 0.5648\n",
            "Epoch: 103, loss:0.2127 Train Acc: 0.5981, Test Acc: 0.5704\n",
            "Epoch: 104, loss:0.1871 Train Acc: 0.5998, Test Acc: 0.5721\n",
            "Epoch: 105, loss:0.0245 Train Acc: 0.5999, Test Acc: 0.5683\n",
            "Epoch: 106, loss:0.2800 Train Acc: 0.6063, Test Acc: 0.5877\n",
            "Epoch: 107, loss:0.1386 Train Acc: 0.6119, Test Acc: 0.6025\n",
            "Epoch: 108, loss:0.1286 Train Acc: 0.6164, Test Acc: 0.6135\n",
            "Epoch: 109, loss:0.1963 Train Acc: 0.6208, Test Acc: 0.6300\n",
            "Epoch: 110, loss:0.2736 Train Acc: 0.6211, Test Acc: 0.6468\n",
            "Epoch: 111, loss:0.1515 Train Acc: 0.6154, Test Acc: 0.6577\n",
            "Epoch: 112, loss:0.1997 Train Acc: 0.6098, Test Acc: 0.6583\n",
            "Epoch: 113, loss:0.1257 Train Acc: 0.6050, Test Acc: 0.6518\n",
            "Epoch: 114, loss:0.0718 Train Acc: 0.6009, Test Acc: 0.6388\n",
            "Epoch: 115, loss:0.1515 Train Acc: 0.6010, Test Acc: 0.6282\n",
            "Epoch: 116, loss:0.1422 Train Acc: 0.6061, Test Acc: 0.6254\n",
            "Epoch: 117, loss:0.1377 Train Acc: 0.6090, Test Acc: 0.6219\n",
            "Epoch: 118, loss:0.2840 Train Acc: 0.6138, Test Acc: 0.6228\n",
            "Epoch: 119, loss:0.3202 Train Acc: 0.6195, Test Acc: 0.6303\n",
            "Epoch: 120, loss:0.1421 Train Acc: 0.6243, Test Acc: 0.6288\n",
            "Epoch: 121, loss:0.0390 Train Acc: 0.6235, Test Acc: 0.6121\n",
            "Epoch: 122, loss:0.1429 Train Acc: 0.6197, Test Acc: 0.5960\n",
            "Epoch: 123, loss:0.1363 Train Acc: 0.6112, Test Acc: 0.5777\n",
            "Epoch: 124, loss:0.0479 Train Acc: 0.5978, Test Acc: 0.5536\n",
            "Epoch: 125, loss:0.1902 Train Acc: 0.5827, Test Acc: 0.5293\n",
            "Epoch: 126, loss:0.1849 Train Acc: 0.5676, Test Acc: 0.5086\n",
            "Epoch: 127, loss:0.0803 Train Acc: 0.5551, Test Acc: 0.4942\n",
            "Epoch: 128, loss:0.2675 Train Acc: 0.5478, Test Acc: 0.4871\n",
            "Epoch: 129, loss:0.0276 Train Acc: 0.5411, Test Acc: 0.4806\n",
            "Epoch: 130, loss:0.1530 Train Acc: 0.5371, Test Acc: 0.4777\n",
            "Epoch: 131, loss:0.2941 Train Acc: 0.5420, Test Acc: 0.4823\n",
            "Epoch: 132, loss:0.2071 Train Acc: 0.5538, Test Acc: 0.4956\n",
            "Epoch: 133, loss:0.0889 Train Acc: 0.5615, Test Acc: 0.5054\n",
            "Epoch: 134, loss:0.4487 Train Acc: 0.5693, Test Acc: 0.5161\n",
            "Epoch: 135, loss:0.2645 Train Acc: 0.5777, Test Acc: 0.5267\n",
            "Epoch: 136, loss:0.1347 Train Acc: 0.5771, Test Acc: 0.5244\n",
            "Epoch: 137, loss:0.1794 Train Acc: 0.5750, Test Acc: 0.5206\n",
            "Epoch: 138, loss:0.3156 Train Acc: 0.5754, Test Acc: 0.5220\n",
            "Epoch: 139, loss:0.2747 Train Acc: 0.5758, Test Acc: 0.5235\n",
            "Epoch: 140, loss:0.3619 Train Acc: 0.5774, Test Acc: 0.5253\n",
            "Epoch: 141, loss:0.1768 Train Acc: 0.5769, Test Acc: 0.5241\n",
            "Epoch: 142, loss:0.1554 Train Acc: 0.5761, Test Acc: 0.5235\n",
            "Epoch: 143, loss:0.0728 Train Acc: 0.5746, Test Acc: 0.5228\n",
            "Epoch: 144, loss:0.1853 Train Acc: 0.5705, Test Acc: 0.5149\n",
            "Epoch: 145, loss:0.2493 Train Acc: 0.5648, Test Acc: 0.5061\n",
            "Epoch: 146, loss:0.0895 Train Acc: 0.5581, Test Acc: 0.4973\n",
            "Epoch: 147, loss:0.0787 Train Acc: 0.5512, Test Acc: 0.4891\n",
            "Epoch: 148, loss:0.0983 Train Acc: 0.5486, Test Acc: 0.4857\n",
            "Epoch: 149, loss:0.0100 Train Acc: 0.5461, Test Acc: 0.4824\n",
            "Epoch: 150, loss:0.1863 Train Acc: 0.5468, Test Acc: 0.4821\n",
            "Epoch: 151, loss:0.1604 Train Acc: 0.5454, Test Acc: 0.4809\n",
            "Epoch: 152, loss:0.1900 Train Acc: 0.5511, Test Acc: 0.4867\n",
            "Epoch: 153, loss:0.2127 Train Acc: 0.5594, Test Acc: 0.4946\n",
            "Epoch: 154, loss:0.1948 Train Acc: 0.5669, Test Acc: 0.5023\n",
            "Epoch: 155, loss:0.0945 Train Acc: 0.5743, Test Acc: 0.5105\n",
            "Epoch: 156, loss:0.0973 Train Acc: 0.5799, Test Acc: 0.5177\n",
            "Epoch: 157, loss:0.1995 Train Acc: 0.5892, Test Acc: 0.5297\n",
            "Epoch: 158, loss:0.1891 Train Acc: 0.5965, Test Acc: 0.5406\n",
            "Epoch: 159, loss:0.2008 Train Acc: 0.6028, Test Acc: 0.5515\n",
            "Epoch: 160, loss:0.2017 Train Acc: 0.6106, Test Acc: 0.5665\n",
            "Epoch: 161, loss:0.1166 Train Acc: 0.6165, Test Acc: 0.5803\n",
            "Epoch: 162, loss:0.1101 Train Acc: 0.6213, Test Acc: 0.5940\n",
            "Epoch: 163, loss:0.1564 Train Acc: 0.6234, Test Acc: 0.6046\n",
            "Epoch: 164, loss:0.1638 Train Acc: 0.6235, Test Acc: 0.6150\n",
            "Epoch: 165, loss:0.1869 Train Acc: 0.6226, Test Acc: 0.6208\n",
            "Epoch: 166, loss:0.2451 Train Acc: 0.6207, Test Acc: 0.6294\n",
            "Epoch: 167, loss:0.0348 Train Acc: 0.6168, Test Acc: 0.6339\n",
            "Epoch: 168, loss:0.2477 Train Acc: 0.6163, Test Acc: 0.6356\n",
            "Epoch: 169, loss:0.2583 Train Acc: 0.6165, Test Acc: 0.6368\n",
            "Epoch: 170, loss:0.3150 Train Acc: 0.6204, Test Acc: 0.6356\n",
            "Epoch: 171, loss:0.1404 Train Acc: 0.6208, Test Acc: 0.6343\n",
            "Epoch: 172, loss:0.1439 Train Acc: 0.6216, Test Acc: 0.6318\n",
            "Epoch: 173, loss:0.1963 Train Acc: 0.6226, Test Acc: 0.6281\n",
            "Epoch: 174, loss:0.0937 Train Acc: 0.6233, Test Acc: 0.6244\n",
            "Epoch: 175, loss:0.0568 Train Acc: 0.6231, Test Acc: 0.6209\n",
            "Epoch: 176, loss:0.0926 Train Acc: 0.6233, Test Acc: 0.6163\n",
            "Epoch: 177, loss:0.1424 Train Acc: 0.6239, Test Acc: 0.6072\n",
            "Epoch: 178, loss:0.0406 Train Acc: 0.6232, Test Acc: 0.5987\n",
            "Epoch: 179, loss:0.3069 Train Acc: 0.6225, Test Acc: 0.5964\n",
            "Epoch: 180, loss:0.1915 Train Acc: 0.6214, Test Acc: 0.5973\n",
            "Epoch: 181, loss:0.3011 Train Acc: 0.6211, Test Acc: 0.5979\n",
            "Epoch: 182, loss:0.1933 Train Acc: 0.6216, Test Acc: 0.5961\n",
            "Epoch: 183, loss:0.1453 Train Acc: 0.6228, Test Acc: 0.5957\n",
            "Epoch: 184, loss:0.0916 Train Acc: 0.6225, Test Acc: 0.5935\n",
            "Epoch: 185, loss:0.3398 Train Acc: 0.6219, Test Acc: 0.5910\n",
            "Epoch: 186, loss:0.1971 Train Acc: 0.6206, Test Acc: 0.5871\n",
            "Epoch: 187, loss:0.1886 Train Acc: 0.6190, Test Acc: 0.5839\n",
            "Epoch: 188, loss:0.1571 Train Acc: 0.6157, Test Acc: 0.5783\n",
            "Epoch: 189, loss:0.1036 Train Acc: 0.6152, Test Acc: 0.5760\n",
            "Epoch: 190, loss:0.2238 Train Acc: 0.6176, Test Acc: 0.5769\n",
            "Epoch: 191, loss:0.1036 Train Acc: 0.6204, Test Acc: 0.5797\n",
            "Epoch: 192, loss:0.1396 Train Acc: 0.6236, Test Acc: 0.5831\n",
            "Epoch: 193, loss:0.1477 Train Acc: 0.6251, Test Acc: 0.5849\n",
            "Epoch: 194, loss:0.2451 Train Acc: 0.6248, Test Acc: 0.5851\n",
            "Epoch: 195, loss:0.1436 Train Acc: 0.6199, Test Acc: 0.5815\n",
            "Epoch: 196, loss:0.1259 Train Acc: 0.6159, Test Acc: 0.5800\n",
            "Epoch: 197, loss:0.2060 Train Acc: 0.6115, Test Acc: 0.5800\n",
            "Epoch: 198, loss:0.2331 Train Acc: 0.6140, Test Acc: 0.5850\n",
            "Epoch: 199, loss:0.0884 Train Acc: 0.6168, Test Acc: 0.5921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(list(zip(train_acc_list2,test_acc_list2, losses2)), columns =['train_acc2', 'test_acc2','loss2'])\n",
        "df2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wpPOAzYiJKCa",
        "outputId": "8371ea3f-e675-4bf9-eb11-66cfda3a3065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_acc2  test_acc2     loss2\n",
              "0    0.504238   0.584167  0.579217\n",
              "1    0.511960   0.545464  0.143792\n",
              "2    0.514905   0.548583  0.509336\n",
              "3    0.515087   0.550671  0.130809\n",
              "4    0.518371   0.546012  0.263370"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5aa7bbc4-706f-4b33-9bf4-c0f223495ef2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_acc2</th>\n",
              "      <th>test_acc2</th>\n",
              "      <th>loss2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.504238</td>\n",
              "      <td>0.584167</td>\n",
              "      <td>0.579217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.511960</td>\n",
              "      <td>0.545464</td>\n",
              "      <td>0.143792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.514905</td>\n",
              "      <td>0.548583</td>\n",
              "      <td>0.509336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.515087</td>\n",
              "      <td>0.550671</td>\n",
              "      <td>0.130809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.518371</td>\n",
              "      <td>0.546012</td>\n",
              "      <td>0.263370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5aa7bbc4-706f-4b33-9bf4-c0f223495ef2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5aa7bbc4-706f-4b33-9bf4-c0f223495ef2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5aa7bbc4-706f-4b33-9bf4-c0f223495ef2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_float = [loss for loss in losses2] \n",
        "loss_indices = [i for i,l in enumerate(losses_float)] \n",
        "fig = px.line(df2, x=loss_indices, y=[\"train_acc2\", \"test_acc2\", \"loss2\"], title=\"Mesure de performance pour le modele 1: 2 couches SAGE et une GAT\",\n",
        "            labels={\"train_acc2\": \"train_acc2\", \"loss2\": \"loss\",\"test_acc2\":\"test_acc2\"})\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "DewF40dhCZmt",
        "outputId": "70f86211-90a3-4c4c-aada-4a9a8bbb0a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"cc8d5c1b-c957-4180-948e-63d7419c0525\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"cc8d5c1b-c957-4180-948e-63d7419c0525\")) {                    Plotly.newPlot(                        \"cc8d5c1b-c957-4180-948e-63d7419c0525\",                        [{\"hovertemplate\":\"variable=train_acc2<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"train_acc2\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train_acc2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[0.5042383283378026,0.5119597219699055,0.5149047800852404,0.5150865506970846,0.5183714034946707,0.5225491031829643,0.5094519308488411,0.4900936041759876,0.4659987844026257,0.44585618110299186,0.4324657460304702,0.4304140474133211,0.4308742920378116,0.430907368035356,0.4331028786805729,0.437123425731173,0.4473705773764585,0.46510461754766125,0.4779988742114559,0.48618713791970763,0.49669395088318163,0.5054349848657769,0.5111846466473625,0.5169061022024488,0.5198618738140826,0.522119486855309,0.5240054210291272,0.5260207885912439,0.5296127188987716,0.5331262843380372,0.5377354201105345,0.542958710902915,0.5488295659075838,0.5541509313257711,0.5557289293721215,0.5528036579054529,0.5403668906009523,0.5319088928375613,0.5202759325047683,0.5089526383496827,0.5040393853754317,0.5086009511739326,0.5185608632186911,0.5321964143722014,0.5423517452029167,0.544280879371978,0.5492382550349332,0.5543743641104231,0.5603914019527475,0.5692948299845029,0.5769505329682475,0.5810488810188832,0.5819062042113371,0.5776225613724429,0.5744156249115753,0.5736749071052728,0.5756778823815989,0.5767289455705178,0.5776173071457893,0.5761112022982858,0.5723184580110912,0.5683529009277375,0.564607162746313,0.5583276314294818,0.5540622246016322,0.5522777482629783,0.5578556993544838,0.5649818531826568,0.5726847416847891,0.5816106091089118,0.5934662973910739,0.5950759489492161,0.593078151008422,0.5885717674049717,0.5845346375024453,0.5823389987053585,0.5806697949734121,0.5804442348677247,0.5826913522708307,0.585261373939774,0.5880616973335799,0.5906027951255539,0.5924477463314732,0.592731577092269,0.5962120921643641,0.5987282131569526,0.5987098746244125,0.5978709540404337,0.5967739996670102,0.5958808195814417,0.595070502494758,0.5938068866149165,0.5925490375692072,0.5919746096289009,0.5917074001655928,0.5925169227106848,0.5948874631794049,0.595951943869071,0.5960197233929038,0.5956819278798186,0.5935900152059883,0.5953446065286508,0.5980690640156521,0.5997922837606361,0.5999168730082637,0.6062765761347182,0.6119323410414462,0.6163871819629422,0.6207844826949867,0.621108719740268,0.6153915060120655,0.6097981126202731,0.6049989276251552,0.6009442152861191,0.6009607981380456,0.6061448616431407,0.6089627546582692,0.6137643232781618,0.6195138697230649,0.624282041965739,0.623489935259726,0.6196813898470092,0.611153510869124,0.5977811708405901,0.5826946585890664,0.5675732633986368,0.5551248727195632,0.5477955084717612,0.5410613968430761,0.5370943403828481,0.5419548844931317,0.5537763818564838,0.5615273785704649,0.5693054409593059,0.5777471121745096,0.5771322138737829,0.5749546701206842,0.5754348551760848,0.5757998188855257,0.5773994361522781,0.5769147145206935,0.5760803176977116,0.5745695609373418,0.5705017386620452,0.5648074256729408,0.5581253949640621,0.5512209951310492,0.5486302256744171,0.5460840530684068,0.5467708189376067,0.5454076034247101,0.5511403491595084,0.5594339408893862,0.5669177409552462,0.5742736070096818,0.5799317427259973,0.5891724664785465,0.5964988191574128,0.6028324049328423,0.6106338678530727,0.6164696733214052,0.6213222848309604,0.6234103273183288,0.6235465783860902,0.6226341498897073,0.6206643018716734,0.616759322177081,0.616317928692609,0.616526547121135,0.6203584418044932,0.620781163561564,0.6215981317302799,0.6226401730275787,0.6232984891817268,0.6231200248881182,0.6232903771683809,0.6238722122867502,0.6232082061895918,0.6224788554541231,0.6214296504673136,0.6210707867868656,0.6215725526171073,0.622796121037698,0.6224629646222923,0.6218848330929546,0.6206010973695855,0.6189705826870721,0.6156650333625451,0.615151810755161,0.6176163634354216,0.62041068932173,0.6236451656193754,0.6251265115256716,0.6248024026322598,0.6198549715543858,0.6159170696444923,0.6115424517933675,0.613951181519229,0.6167525173128049],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=test_acc2<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"test_acc2\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test_acc2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[0.5841673265223354,0.5454643774503177,0.5485834025377083,0.550671121497132,0.5460118967148844,0.5738079144054539,0.5849823287433129,0.5751955425944881,0.5421908495722204,0.49504142606075824,0.4608451302651654,0.46120628053844226,0.467009791614361,0.47446455126595727,0.4873037331736805,0.5004190888197918,0.5216709476814925,0.540609127252361,0.5490498078371541,0.5505803511075918,0.5518163734332451,0.5485505706946832,0.5466946059213195,0.5400722300546553,0.5289036095714479,0.5162961818497847,0.5184611522045617,0.5247996291933024,0.5321307866123333,0.5388883524208656,0.5541435717182641,0.569939550783136,0.5809555997605206,0.5903281253017633,0.5930348210664554,0.5861990382201279,0.5714420904227583,0.5645358156781707,0.556171420846289,0.5468500743544681,0.5409374456826126,0.5484598003051431,0.5607389095965547,0.5756503601846308,0.5864307924061878,0.5895807180517199,0.5950868112555283,0.6014658452268294,0.6088336970586531,0.6205585275884046,0.6276579308213753,0.6261669788910561,0.6118310511983622,0.6079008864597617,0.6100716506691902,0.6092972054307731,0.6133731821781031,0.6148708936055157,0.6256040093474189,0.636624886537013,0.6391664574441376,0.6372332412754205,0.6338283860252225,0.6248537051700496,0.6178856293091793,0.6165742868730567,0.6207690376407423,0.6261872573823364,0.63001313273721,0.629492651461017,0.612899051739122,0.600960814229707,0.6163348075474614,0.6272175978678616,0.632851155874003,0.6349929508101742,0.636369956932347,0.6370710133451785,0.6381390138859382,0.6394658066011317,0.6389868479499411,0.6364443114003747,0.6330153150891289,0.6321694123100098,0.6338573552984801,0.6310202978041292,0.6290745282836672,0.6257894126962669,0.6185673728731725,0.6212962784140288,0.6196875181057958,0.6170947681492497,0.6111415824948339,0.605199984549721,0.5960949419648892,0.5944205179706059,0.6020046737094189,0.6006836748488771,0.5907201761331815,0.577570057359161,0.561660132486143,0.5647791575735337,0.5704300971436297,0.5721315591262868,0.5682892678499005,0.5877102686417274,0.6025396396222407,0.6135286506112516,0.629990922961046,0.6467863419533015,0.6576913420498658,0.6582958342185057,0.6518231329303384,0.6387859943220225,0.628242144498735,0.6254215029258965,0.6218775951640626,0.6228200621873733,0.6303250352459491,0.6288167017516755,0.6120917746576797,0.595959752023021,0.5777168350103324,0.5535834991019525,0.529307248112169,0.508592286448174,0.494175244790359,0.4870681164178528,0.4806272813302691,0.47772166322254206,0.4823190868885069,0.4955599760520675,0.5054047007474073,0.5160682902334923,0.5266517313968984,0.5243766778037429,0.5206203287046872,0.5219896096873249,0.5234960118967149,0.5253249386817049,0.5241043666351224,0.5234786303327603,0.5227640549257421,0.5148554433264451,0.5061086540875644,0.497278819598679,0.4890515459935495,0.48573359856312404,0.48236736901060273,0.4821298209698912,0.4809227679174955,0.48674366055736884,0.49458660847061553,0.5022750535931555,0.5104888082040983,0.5176857413237026,0.529666467100562,0.5406110585372448,0.5514812955059001,0.5664844821259584,0.5803095849668785,0.5940323297089554,0.6046466714305028,0.6150447092450607,0.6207980069139999,0.6294105718534541,0.6338573552984801,0.6356264122520714,0.6368296027346994,0.6355916491241622,0.6342725815485042,0.6317763958361498,0.6281040576295409,0.6243805403735105,0.6209090557948204,0.6163425326869967,0.6071872766951854,0.5987060391278317,0.5964232603951409,0.5972817165260047,0.5978504799242936,0.5960872168253539,0.5956797157148651,0.5935475772031132,0.5909751057378474,0.5871125359701809,0.5839220533420885,0.5782769076266441,0.5759526062689507,0.5768921763649356,0.5797176461499837,0.5830751849205277,0.5849099055601692,0.5850537862840146,0.5814654589698527,0.579978369609301,0.5799880260337201,0.5850479924293632,0.5921473956623342],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=loss2<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"loss2\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"loss2\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198],\"xaxis\":\"x\",\"y\":[0.5792171955108643,0.14379197359085083,0.5093359351158142,0.13080886006355286,0.2633701264858246,0.22962361574172974,0.1528118997812271,0.2360299974679947,0.27523335814476013,0.14165274798870087,0.1326579451560974,0.3358794152736664,0.08997607231140137,0.18081115186214447,0.21980859339237213,0.14265605807304382,0.24719169735908508,0.1463189423084259,0.1003461703658104,0.17224468290805817,0.31648263335227966,0.1531931608915329,0.2761848270893097,0.15122641623020172,0.058335475623607635,0.089325450360775,0.3910152316093445,0.197133868932724,0.26973485946655273,0.2429042011499405,0.21579118072986603,0.12102769315242767,0.21142441034317017,0.22581353783607483,0.19295451045036316,0.18454648554325104,0.16464358568191528,0.24595287442207336,0.07324470579624176,0.07340120524168015,0.09097576141357422,0.33724740147590637,0.21082723140716553,0.2665182650089264,0.2372576743364334,0.11168090254068375,0.1798826903104782,0.10876588523387909,0.1715448498725891,0.3146629333496094,0.062400445342063904,0.11113431304693222,0.02561492659151554,0.2115919142961502,0.08473219722509384,0.1821995973587036,0.26362431049346924,0.01541072130203247,0.16791389882564545,0.26633748412132263,0.25821033120155334,0.08884205669164658,0.07131004333496094,0.23776081204414368,0.15678173303604126,0.14982736110687256,0.2873397171497345,0.21484728157520294,0.28070423007011414,0.046728797256946564,0.2076573669910431,0.1946294903755188,0.227197527885437,0.16611208021640778,0.24484825134277344,0.11986024677753448,0.11765177547931671,0.09578236937522888,0.21247881650924683,0.24214285612106323,0.03886040300130844,0.08926177024841309,0.09535029530525208,0.14165139198303223,0.2829815745353699,0.0702250674366951,0.13265927135944366,0.21024800837039948,0.0745626762509346,0.23697881400585175,0.04246959835290909,0.09776708483695984,0.09466546028852463,0.20734086632728577,0.08494102954864502,0.25496023893356323,0.1473018378019333,0.09775557368993759,0.09196913242340088,0.12923850119113922,0.033626407384872437,0.15630921721458435,0.21272693574428558,0.1870681345462799,0.024539945647120476,0.28002655506134033,0.13856595754623413,0.1285964995622635,0.19625592231750488,0.27360212802886963,0.15145187079906464,0.19973860681056976,0.12572261691093445,0.07179995626211166,0.1515457034111023,0.14215190708637238,0.13765646517276764,0.28399384021759033,0.32017162442207336,0.14210881292819977,0.03897223249077797,0.14286212623119354,0.13632303476333618,0.04787367209792137,0.19024275243282318,0.18494592607021332,0.08032166212797165,0.26749417185783386,0.027625801041722298,0.1529637724161148,0.2941160202026367,0.20710524916648865,0.08887264132499695,0.4487023949623108,0.264480322599411,0.13470134139060974,0.17943337559700012,0.3156455159187317,0.2747056484222412,0.3618602752685547,0.17676912248134613,0.1553514152765274,0.07275689393281937,0.18525391817092896,0.2493262141942978,0.08947562426328659,0.07867440581321716,0.09833119064569473,0.010035274550318718,0.18626248836517334,0.1603817343711853,0.19001249969005585,0.21266305446624756,0.19482219219207764,0.09445590525865555,0.09733907133340836,0.1995067596435547,0.18914362788200378,0.200790673494339,0.20169921219348907,0.11655187606811523,0.11012672632932663,0.15639719367027283,0.16376812756061554,0.18690745532512665,0.24513478577136993,0.034798696637153625,0.24771808087825775,0.2583191990852356,0.3150128126144409,0.14037515223026276,0.1439063996076584,0.196318581700325,0.0937129557132721,0.05679654702544212,0.09256734699010849,0.14243942499160767,0.040619030594825745,0.30694884061813354,0.19149480760097504,0.3011253774166107,0.19325187802314758,0.14534929394721985,0.09155276417732239,0.3397582173347473,0.19706228375434875,0.18855327367782593,0.15706679224967957,0.10356795787811279,0.22377654910087585,0.10355944186449051,0.13960100710391998,0.14767055213451385,0.24512596428394318,0.14363475143909454,0.1259194165468216,0.20604729652404785,0.233062282204628,0.08838073909282684],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Mesure de performance pour le modele 1: 2 couches SAGE et une GAT\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cc8d5c1b-c957-4180-948e-63d7419c0525');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPT75V4VCZUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnlwmZawCZQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model3"
      ],
      "metadata": {
        "id": "u__ScmPSOXlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,SAGEConv,GATConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(24, hidden_channels)\n",
        "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
        "        # self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        # self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self,x,edge_index,batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x,batch)  \n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model3 = GCN(hidden_channels=64)\n",
        "print(model3)"
      ],
      "metadata": {
        "id": "J2NKL0Z6G6xp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4bf32e-7aa3-4918-82dc-fe1eec1cf9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GATConv(24, 64, heads=1)\n",
            "  (conv2): GATConv(64, 64, heads=1)\n",
            "  (conv3): GATConv(64, 64, heads=1)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model3.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model3 = model3.to(device)\n",
        "dataset.data = dataset.data.to(device)\n",
        "# optimizer = torch.optim.Adam(list(model.parameters())+list(atom_encoder.parameters())+list(bond_encoder.parameters()), \n",
        "#                           lr=0.01)\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def train():\n",
        "  model3.train()\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    data.x = atom_encoder(data.x)\n",
        "    # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "  # Iterate in batches over the training dataset.\n",
        "    optimizer.zero_grad()\n",
        "    out = model3(data.x, data.edge_index, data.batch)  \n",
        "    loss = criterion(out, data.y.squeeze(1)) \n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "    return float(loss)"
      ],
      "metadata": {
        "id": "LAEA9VUiG6vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def test(test_loader):\n",
        "    model3.eval()\n",
        "    y_label = []\n",
        "    y_predi = []\n",
        "    for data in test_loader:\n",
        "      data = data.to(device)\n",
        "      data.x = atom_encoder(data.x)\n",
        "      # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "    # Iterate in batches over the training dataset.\n",
        "      out = model3(data.x,data.edge_index,data.batch)  \n",
        "      y_predi_loader = out[:,1]\n",
        "      y_label.append(data.y.view(y_predi_loader.shape).detach().cpu())\n",
        "      y_predi.append(y_predi_loader.detach().cpu())\n",
        "    \n",
        "    y_label = torch.cat(y_label, dim = 0).numpy()\n",
        "    y_predi = torch.cat(y_predi, dim = 0).numpy()\n",
        "    # # y_label = np.array(y_label)\n",
        "    # # y_predi = np.array(y_label)\n",
        "    # evaluator = Evaluator(name='ogbg-molhiv')\n",
        "    # y_label = torch.tensor(y_label)\n",
        "    # y_predi = torch.tensor(y_predi)\n",
        "\n",
        "    precision = evaluator.eval({'y_true': y_label.reshape(len(y_label),1),\n",
        "                                'y_pred': y_predi.reshape(len(y_label),1),\n",
        "                               })\n",
        "    \n",
        "    return precision['rocauc']"
      ],
      "metadata": {
        "id": "CyvpEf7hG6sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses3 = []\n",
        "train_acc_list3 = []\n",
        "test_acc_list3 = []\n",
        "\n",
        "for epoch in range(1, 150):\n",
        "    loss = train()\n",
        "    losses3.append(loss)\n",
        "    train_acc = test(train_loader)\n",
        "    train_acc_list3.append(train_acc)\n",
        "    test_acc = test(test_loader)\n",
        "    test_acc_list3.append(test_acc)\n",
        "    # if epoch % 10==0:\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "d2gfYVqhG6py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55e82f3-3f4f-47b2-e27f-72fedebedb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.5194, Test Acc: 0.5487\n",
            "Epoch: 002, Train Acc: 0.5306, Test Acc: 0.5939\n",
            "Epoch: 003, Train Acc: 0.5323, Test Acc: 0.6056\n",
            "Epoch: 004, Train Acc: 0.5295, Test Acc: 0.6038\n",
            "Epoch: 005, Train Acc: 0.5276, Test Acc: 0.6047\n",
            "Epoch: 006, Train Acc: 0.5285, Test Acc: 0.6053\n",
            "Epoch: 007, Train Acc: 0.5266, Test Acc: 0.6047\n",
            "Epoch: 008, Train Acc: 0.5260, Test Acc: 0.6021\n",
            "Epoch: 009, Train Acc: 0.5228, Test Acc: 0.5972\n",
            "Epoch: 010, Train Acc: 0.5167, Test Acc: 0.5888\n",
            "Epoch: 011, Train Acc: 0.5115, Test Acc: 0.5767\n",
            "Epoch: 012, Train Acc: 0.5076, Test Acc: 0.5663\n",
            "Epoch: 013, Train Acc: 0.5035, Test Acc: 0.5558\n",
            "Epoch: 014, Train Acc: 0.5089, Test Acc: 0.5584\n",
            "Epoch: 015, Train Acc: 0.5129, Test Acc: 0.5624\n",
            "Epoch: 016, Train Acc: 0.5148, Test Acc: 0.5640\n",
            "Epoch: 017, Train Acc: 0.5179, Test Acc: 0.5677\n",
            "Epoch: 018, Train Acc: 0.5218, Test Acc: 0.5703\n",
            "Epoch: 019, Train Acc: 0.5242, Test Acc: 0.5762\n",
            "Epoch: 020, Train Acc: 0.5268, Test Acc: 0.5819\n",
            "Epoch: 021, Train Acc: 0.5305, Test Acc: 0.5852\n",
            "Epoch: 022, Train Acc: 0.5331, Test Acc: 0.5849\n",
            "Epoch: 023, Train Acc: 0.5356, Test Acc: 0.5830\n",
            "Epoch: 024, Train Acc: 0.5364, Test Acc: 0.5849\n",
            "Epoch: 025, Train Acc: 0.5375, Test Acc: 0.5865\n",
            "Epoch: 026, Train Acc: 0.5381, Test Acc: 0.5896\n",
            "Epoch: 027, Train Acc: 0.5393, Test Acc: 0.5912\n",
            "Epoch: 028, Train Acc: 0.5422, Test Acc: 0.5918\n",
            "Epoch: 029, Train Acc: 0.5453, Test Acc: 0.5916\n",
            "Epoch: 030, Train Acc: 0.5475, Test Acc: 0.5935\n",
            "Epoch: 031, Train Acc: 0.5487, Test Acc: 0.5951\n",
            "Epoch: 032, Train Acc: 0.5515, Test Acc: 0.5979\n",
            "Epoch: 033, Train Acc: 0.5540, Test Acc: 0.5959\n",
            "Epoch: 034, Train Acc: 0.5558, Test Acc: 0.5894\n",
            "Epoch: 035, Train Acc: 0.5565, Test Acc: 0.5788\n",
            "Epoch: 036, Train Acc: 0.5568, Test Acc: 0.5680\n",
            "Epoch: 037, Train Acc: 0.5564, Test Acc: 0.5591\n",
            "Epoch: 038, Train Acc: 0.5567, Test Acc: 0.5557\n",
            "Epoch: 039, Train Acc: 0.5561, Test Acc: 0.5510\n",
            "Epoch: 040, Train Acc: 0.5546, Test Acc: 0.5453\n",
            "Epoch: 041, Train Acc: 0.5539, Test Acc: 0.5426\n",
            "Epoch: 042, Train Acc: 0.5544, Test Acc: 0.5428\n",
            "Epoch: 043, Train Acc: 0.5563, Test Acc: 0.5467\n",
            "Epoch: 044, Train Acc: 0.5590, Test Acc: 0.5589\n",
            "Epoch: 045, Train Acc: 0.5608, Test Acc: 0.5696\n",
            "Epoch: 046, Train Acc: 0.5628, Test Acc: 0.5848\n",
            "Epoch: 047, Train Acc: 0.5646, Test Acc: 0.6042\n",
            "Epoch: 048, Train Acc: 0.5659, Test Acc: 0.6174\n",
            "Epoch: 049, Train Acc: 0.5671, Test Acc: 0.6275\n",
            "Epoch: 050, Train Acc: 0.5692, Test Acc: 0.6360\n",
            "Epoch: 051, Train Acc: 0.5720, Test Acc: 0.6426\n",
            "Epoch: 052, Train Acc: 0.5750, Test Acc: 0.6465\n",
            "Epoch: 053, Train Acc: 0.5782, Test Acc: 0.6485\n",
            "Epoch: 054, Train Acc: 0.5797, Test Acc: 0.6511\n",
            "Epoch: 055, Train Acc: 0.5805, Test Acc: 0.6524\n",
            "Epoch: 056, Train Acc: 0.5821, Test Acc: 0.6548\n",
            "Epoch: 057, Train Acc: 0.5822, Test Acc: 0.6557\n",
            "Epoch: 058, Train Acc: 0.5828, Test Acc: 0.6558\n",
            "Epoch: 059, Train Acc: 0.5837, Test Acc: 0.6581\n",
            "Epoch: 060, Train Acc: 0.5816, Test Acc: 0.6561\n",
            "Epoch: 061, Train Acc: 0.5803, Test Acc: 0.6551\n",
            "Epoch: 062, Train Acc: 0.5803, Test Acc: 0.6557\n",
            "Epoch: 063, Train Acc: 0.5796, Test Acc: 0.6540\n",
            "Epoch: 064, Train Acc: 0.5795, Test Acc: 0.6514\n",
            "Epoch: 065, Train Acc: 0.5790, Test Acc: 0.6472\n",
            "Epoch: 066, Train Acc: 0.5796, Test Acc: 0.6439\n",
            "Epoch: 067, Train Acc: 0.5807, Test Acc: 0.6404\n",
            "Epoch: 068, Train Acc: 0.5827, Test Acc: 0.6358\n",
            "Epoch: 069, Train Acc: 0.5836, Test Acc: 0.6305\n",
            "Epoch: 070, Train Acc: 0.5852, Test Acc: 0.6250\n",
            "Epoch: 071, Train Acc: 0.5852, Test Acc: 0.6254\n",
            "Epoch: 072, Train Acc: 0.5843, Test Acc: 0.6269\n",
            "Epoch: 073, Train Acc: 0.5842, Test Acc: 0.6288\n",
            "Epoch: 074, Train Acc: 0.5831, Test Acc: 0.6301\n",
            "Epoch: 075, Train Acc: 0.5821, Test Acc: 0.6329\n",
            "Epoch: 076, Train Acc: 0.5807, Test Acc: 0.6333\n",
            "Epoch: 077, Train Acc: 0.5779, Test Acc: 0.6327\n",
            "Epoch: 078, Train Acc: 0.5763, Test Acc: 0.6340\n",
            "Epoch: 079, Train Acc: 0.5748, Test Acc: 0.6376\n",
            "Epoch: 080, Train Acc: 0.5743, Test Acc: 0.6412\n",
            "Epoch: 081, Train Acc: 0.5740, Test Acc: 0.6425\n",
            "Epoch: 082, Train Acc: 0.5739, Test Acc: 0.6429\n",
            "Epoch: 083, Train Acc: 0.5743, Test Acc: 0.6456\n",
            "Epoch: 084, Train Acc: 0.5747, Test Acc: 0.6494\n",
            "Epoch: 085, Train Acc: 0.5737, Test Acc: 0.6507\n",
            "Epoch: 086, Train Acc: 0.5762, Test Acc: 0.6597\n",
            "Epoch: 087, Train Acc: 0.5783, Test Acc: 0.6657\n",
            "Epoch: 088, Train Acc: 0.5790, Test Acc: 0.6700\n",
            "Epoch: 089, Train Acc: 0.5794, Test Acc: 0.6727\n",
            "Epoch: 090, Train Acc: 0.5798, Test Acc: 0.6775\n",
            "Epoch: 091, Train Acc: 0.5792, Test Acc: 0.6818\n",
            "Epoch: 092, Train Acc: 0.5782, Test Acc: 0.6867\n",
            "Epoch: 093, Train Acc: 0.5778, Test Acc: 0.6903\n",
            "Epoch: 094, Train Acc: 0.5777, Test Acc: 0.6924\n",
            "Epoch: 095, Train Acc: 0.5778, Test Acc: 0.6935\n",
            "Epoch: 096, Train Acc: 0.5781, Test Acc: 0.6940\n",
            "Epoch: 097, Train Acc: 0.5810, Test Acc: 0.6932\n",
            "Epoch: 098, Train Acc: 0.5830, Test Acc: 0.6920\n",
            "Epoch: 099, Train Acc: 0.5840, Test Acc: 0.6908\n",
            "Epoch: 100, Train Acc: 0.5845, Test Acc: 0.6893\n",
            "Epoch: 101, Train Acc: 0.5855, Test Acc: 0.6890\n",
            "Epoch: 102, Train Acc: 0.5859, Test Acc: 0.6896\n",
            "Epoch: 103, Train Acc: 0.5861, Test Acc: 0.6898\n",
            "Epoch: 104, Train Acc: 0.5873, Test Acc: 0.6903\n",
            "Epoch: 105, Train Acc: 0.5881, Test Acc: 0.6906\n",
            "Epoch: 106, Train Acc: 0.5895, Test Acc: 0.6906\n",
            "Epoch: 107, Train Acc: 0.5925, Test Acc: 0.6929\n",
            "Epoch: 108, Train Acc: 0.5981, Test Acc: 0.6946\n",
            "Epoch: 109, Train Acc: 0.6008, Test Acc: 0.6915\n",
            "Epoch: 110, Train Acc: 0.6004, Test Acc: 0.6841\n",
            "Epoch: 111, Train Acc: 0.6004, Test Acc: 0.6877\n",
            "Epoch: 112, Train Acc: 0.6004, Test Acc: 0.6927\n",
            "Epoch: 113, Train Acc: 0.5991, Test Acc: 0.6959\n",
            "Epoch: 114, Train Acc: 0.5945, Test Acc: 0.6988\n",
            "Epoch: 115, Train Acc: 0.5906, Test Acc: 0.6993\n",
            "Epoch: 116, Train Acc: 0.5887, Test Acc: 0.6972\n",
            "Epoch: 117, Train Acc: 0.5883, Test Acc: 0.6950\n",
            "Epoch: 118, Train Acc: 0.5880, Test Acc: 0.6929\n",
            "Epoch: 119, Train Acc: 0.5876, Test Acc: 0.6912\n",
            "Epoch: 120, Train Acc: 0.5875, Test Acc: 0.6894\n",
            "Epoch: 121, Train Acc: 0.5871, Test Acc: 0.6890\n",
            "Epoch: 122, Train Acc: 0.5864, Test Acc: 0.6880\n",
            "Epoch: 123, Train Acc: 0.5855, Test Acc: 0.6873\n",
            "Epoch: 124, Train Acc: 0.5847, Test Acc: 0.6875\n",
            "Epoch: 125, Train Acc: 0.5839, Test Acc: 0.6880\n",
            "Epoch: 126, Train Acc: 0.5832, Test Acc: 0.6891\n",
            "Epoch: 127, Train Acc: 0.5826, Test Acc: 0.6896\n",
            "Epoch: 128, Train Acc: 0.5820, Test Acc: 0.6905\n",
            "Epoch: 129, Train Acc: 0.5814, Test Acc: 0.6911\n",
            "Epoch: 130, Train Acc: 0.5805, Test Acc: 0.6932\n",
            "Epoch: 131, Train Acc: 0.5801, Test Acc: 0.6938\n",
            "Epoch: 132, Train Acc: 0.5794, Test Acc: 0.6949\n",
            "Epoch: 133, Train Acc: 0.5800, Test Acc: 0.6937\n",
            "Epoch: 134, Train Acc: 0.5798, Test Acc: 0.6942\n",
            "Epoch: 135, Train Acc: 0.5803, Test Acc: 0.6933\n",
            "Epoch: 136, Train Acc: 0.5809, Test Acc: 0.6914\n",
            "Epoch: 137, Train Acc: 0.5808, Test Acc: 0.6937\n",
            "Epoch: 138, Train Acc: 0.5813, Test Acc: 0.6935\n",
            "Epoch: 139, Train Acc: 0.5813, Test Acc: 0.6943\n",
            "Epoch: 140, Train Acc: 0.5813, Test Acc: 0.6938\n",
            "Epoch: 141, Train Acc: 0.5813, Test Acc: 0.6924\n",
            "Epoch: 142, Train Acc: 0.5816, Test Acc: 0.6905\n",
            "Epoch: 143, Train Acc: 0.5822, Test Acc: 0.6885\n",
            "Epoch: 144, Train Acc: 0.5826, Test Acc: 0.6856\n",
            "Epoch: 145, Train Acc: 0.5831, Test Acc: 0.6776\n",
            "Epoch: 146, Train Acc: 0.5836, Test Acc: 0.6644\n",
            "Epoch: 147, Train Acc: 0.5825, Test Acc: 0.6467\n",
            "Epoch: 148, Train Acc: 0.5807, Test Acc: 0.6282\n",
            "Epoch: 149, Train Acc: 0.5804, Test Acc: 0.6173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = pd.DataFrame(list(zip(train_acc_list3,test_acc_list3, losses3)), columns =['train_acc3', 'test_acc3','loss3'])\n",
        "df3.head(3)"
      ],
      "metadata": {
        "id": "WTrSHmgiG6nC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "312072e6-231a-469f-d964-46d2a973571e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_acc3  test_acc3     loss3\n",
              "0    0.519387   0.548713  0.633775\n",
              "1    0.530647   0.593859  0.192105\n",
              "2    0.532333   0.605603  0.211480"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e97717f-f130-4ec7-85af-a42ed9dfc07e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_acc3</th>\n",
              "      <th>test_acc3</th>\n",
              "      <th>loss3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.519387</td>\n",
              "      <td>0.548713</td>\n",
              "      <td>0.633775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.530647</td>\n",
              "      <td>0.593859</td>\n",
              "      <td>0.192105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.532333</td>\n",
              "      <td>0.605603</td>\n",
              "      <td>0.211480</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e97717f-f130-4ec7-85af-a42ed9dfc07e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e97717f-f130-4ec7-85af-a42ed9dfc07e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e97717f-f130-4ec7-85af-a42ed9dfc07e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses_float = [loss for loss in losses3] \n",
        "loss_indices = [i for i,l in enumerate(losses_float)] \n",
        "fig = px.line(df3, x=loss_indices, y=[\"train_acc3\", \"test_acc3\", \"loss3\"], title=\"Mesure de performance pour le modele 3: avec 3 couches de GAT\",\n",
        "            labels={\"train_acc3\": \"train_acc3\", \"loss3\": \"loss3\",\"test_acc3\":\"test_acc3\"})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "V9AAPVdWG6if",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "9cdfebd4-2994-4371-ea08-ccb39411e80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"42bcd5ad-7d80-4b01-b72a-fefaab173ae7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"42bcd5ad-7d80-4b01-b72a-fefaab173ae7\")) {                    Plotly.newPlot(                        \"42bcd5ad-7d80-4b01-b72a-fefaab173ae7\",                        [{\"hovertemplate\":\"variable=train_acc3<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"train_acc3\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train_acc3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\"xaxis\":\"x\",\"y\":[0.519386532899353,0.5306473273876333,0.5323327167007106,0.5295247810858503,0.527622417842349,0.5284715264999613,0.5265831061798727,0.5260200965711481,0.5228011010193507,0.5167282786681883,0.5115145736356542,0.5076341093937166,0.5035112971511737,0.5089275333984277,0.5129170933269579,0.5148450997595666,0.5179462852976383,0.5218272877774284,0.5242180506111715,0.5267933136915817,0.530547189516726,0.5330918755610489,0.5355690896460261,0.536415712157368,0.5374677108549349,0.5381383552189388,0.5393438414107286,0.5422075102736791,0.5453353001398803,0.547463197858695,0.5486712599030639,0.5515499481651318,0.5539884603854891,0.5557801132288407,0.5565412199975969,0.5567671132981452,0.5564293562306208,0.5566848397978604,0.5560768488829053,0.5546124062082097,0.5538561563953114,0.5544380940351763,0.5563346007382368,0.5589991344110119,0.560757467768267,0.5627520875426951,0.5646322548823812,0.5658557464118502,0.5670589002396131,0.5691817872203264,0.5720066645123483,0.5749803517553526,0.5782176602093161,0.5796544348953645,0.580450591200457,0.5820912170654822,0.5821984289195916,0.5827749841809332,0.5837409929739968,0.5816452101137045,0.5803070739216892,0.5803395476054465,0.5796198467057588,0.5794746506374993,0.5790013345223093,0.5795764929282723,0.5807175956207739,0.5827492384703301,0.583603908919083,0.5852307969036867,0.5851688995506688,0.5843323241459036,0.5842223313962239,0.5831369235062515,0.5821073001251172,0.580706894939662,0.577934957184973,0.5762880006175894,0.574774860232445,0.5742698649750894,0.5740054748529124,0.5738990062796467,0.5742705441799982,0.5747223948570297,0.5736663465603833,0.5761642315419274,0.578322770372764,0.5790031799092316,0.579368976605825,0.5797994899965676,0.5792189492120813,0.5782273997514058,0.5778438027601247,0.5776704004653656,0.577760196480396,0.5780695422784295,0.58096128921601,0.5830092714289405,0.5840442387430372,0.5845139537906913,0.5854913168394017,0.5859234577588883,0.586062156527359,0.5873112015396268,0.5881421254469424,0.5895298307821201,0.592478182400504,0.5981252586104729,0.6008151407230554,0.6004461274145351,0.6003940593099155,0.600422598731276,0.5991106952269682,0.5945391463978252,0.590572179643906,0.5887461052083791,0.5883094533430825,0.5880177925030541,0.5875740410241815,0.5875345958787179,0.5871258683058077,0.5863811906067344,0.5854850245826042,0.5846956731417876,0.5839157280481999,0.5831744591888581,0.5825924446578714,0.5819772644230315,0.5813798460373187,0.5804549227336496,0.5800752856351391,0.5794476746689479,0.5799873350070309,0.5798040137575646,0.5802549032955739,0.5808893575716021,0.5807849394282499,0.5812981235900732,0.5813271499885381,0.5813197940712229,0.5812528347193556,0.5816198104131493,0.5822456785139141,0.5825598043766836,0.5831267098022442,0.5835951945919501,0.5824818239640306,0.5807353446547137,0.5803554384372771],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=test_acc3<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"test_acc3\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test_acc3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\"xaxis\":\"x\",\"y\":[0.5487127986249252,0.5938594797118524,0.6056026574480001,0.6038258753548738,0.6047277853956237,0.6052878580119352,0.6046756407037602,0.6020877189594237,0.5972286061916994,0.5887531624789974,0.5767299484346936,0.5662932849224589,0.555754263311381,0.5583595666196721,0.5623882268873481,0.564034647250816,0.5677205044516118,0.5703499488209507,0.5761795322428012,0.5818884103594121,0.5852160142142567,0.5849437030456364,0.5830297997257576,0.584935977906101,0.5864964560922381,0.5896048591127677,0.591224241487862,0.5918499777902239,0.5915931169006741,0.5934741883775275,0.5951041928194828,0.5979431815987176,0.5959481643137179,0.5893740705691496,0.5787906294057437,0.567959983777207,0.5591108364394831,0.5557349504625426,0.5509714362965681,0.5452789741014697,0.5425558624152649,0.5428339674385368,0.5466675679329458,0.5589254330906351,0.5695996446435814,0.5847901658973715,0.6042044071921049,0.6174124645126403,0.6275343285888102,0.6360194287259314,0.6425597249850326,0.646536240560845,0.6485467081249154,0.6510824851773885,0.6524401784507232,0.6548214527124896,0.6556828057706793,0.655765851020684,0.6581471252824505,0.6561096197300063,0.6551420460032059,0.655719500183472,0.6539658935089515,0.651400181540779,0.6471851522818132,0.6439376967496476,0.6403619227872304,0.6357635334788235,0.6305480986500319,0.6249618571235442,0.6253664613067074,0.6268892794376099,0.6287722821993473,0.6301029374843083,0.6329380636937754,0.6332702446937948,0.6327352787809729,0.6339867513856969,0.637621429537071,0.6411730624384404,0.6425191680024721,0.6428861121304004,0.6456043956043956,0.6494235114621758,0.650671121497132,0.659657390061608,0.6656501670561425,0.6700380463122115,0.6726549373298054,0.6775333629463681,0.6817763958361499,0.6866915158655054,0.6902508739064099,0.6924081191216516,0.6935031576507851,0.6940439174182584,0.6932173274879778,0.6920315185693042,0.6908370188686533,0.6892987504586802,0.6890051951563374,0.6896174124645126,0.6898298538017343,0.6902643929005968,0.6906274744587574,0.6906274744587573,0.6929005967670291,0.6946310280229437,0.6915448347785782,0.6840591745688407,0.6877334440603333,0.692717124703065,0.6959288514648796,0.6987929469476042,0.6993095656540297,0.6972344000463508,0.6950211475694779,0.6928938372699357,0.691233897912281,0.6893740705691496,0.6889588443191256,0.6880076865138376,0.6872718669730972,0.6875229340079956,0.6880434152841886,0.6890959655458776,0.6895710616273008,0.6904507618918867,0.6911373336680893,0.6931671140809981,0.6938314760810367,0.6949419648892408,0.6937059425635876,0.6942467023310609,0.6932984414530987,0.6913941945576393,0.6936818015025397,0.693479982232179,0.6943142973019949,0.6938227852990595,0.6924264663280482,0.6904826280924699,0.6885262365051469,0.6856389656038162,0.677571988644045,0.6644276637246761,0.6467177813399254,0.6282083470132679,0.6173294192626354],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=loss3<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"loss3\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"loss3\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\"xaxis\":\"x\",\"y\":[0.6337754726409912,0.1921054571866989,0.21147960424423218,0.5530722141265869,0.3270168900489807,0.23668387532234192,0.29086774587631226,0.2308722585439682,0.2562323808670044,0.18615853786468506,0.13352546095848083,0.21596574783325195,0.1519065499305725,0.3334020674228668,0.14588667452335358,0.1463845819234848,0.2563144564628601,0.07250907272100449,0.18849608302116394,0.08252152055501938,0.1652836799621582,0.0849868431687355,0.09576655179262161,0.25516751408576965,0.09344311058521271,0.22997233271598816,0.09805662930011749,0.21184268593788147,0.19996467232704163,0.1363983005285263,0.09850720316171646,0.3192383944988251,0.18782787024974823,0.28719550371170044,0.14691609144210815,0.1449144333600998,0.08598383516073227,0.22643250226974487,0.1695787012577057,0.09274297207593918,0.20078548789024353,0.17582117021083832,0.206253781914711,0.23086489737033844,0.13517804443836212,0.20910634100437164,0.12376619130373001,0.27299049496650696,0.1462356448173523,0.05743255838751793,0.14701494574546814,0.2297215610742569,0.08946921676397324,0.14953672885894775,0.13838554918766022,0.15302616357803345,0.2672916054725647,0.19479544460773468,0.10544992238283157,0.2310788780450821,0.12309840321540833,0.09544570744037628,0.2870243787765503,0.14971806108951569,0.26853272318840027,0.1326494663953781,0.12362648546695709,0.15261057019233704,0.15286146104335785,0.03164348006248474,0.37758752703666687,0.20599770545959473,0.07259106636047363,0.21362987160682678,0.21430765092372894,0.12985195219516754,0.16638100147247314,0.22280307114124298,0.202035591006279,0.11030706763267517,0.10270936787128448,0.17724116146564484,0.10464750230312347,0.21021871268749237,0.06833796948194504,0.2888903319835663,0.20604442059993744,0.1683964729309082,0.24353396892547607,0.15824207663536072,0.0634392648935318,0.06678783893585205,0.17636476457118988,0.08754315227270126,0.08627323806285858,0.21686431765556335,0.21344809234142303,0.020402071997523308,0.02081410028040409,0.2121812254190445,0.09348452091217041,0.10499314218759537,0.12100489437580109,0.2878960967063904,0.021456321701407433,0.12472809851169586,0.26676061749458313,0.19926674664020538,0.21991349756717682,0.3179164528846741,0.09661345928907394,0.139685720205307,0.20989802479743958,0.06878484040498734,0.19117091596126556,0.2892901599407196,0.18718889355659485,0.06674318015575409,0.0859856903553009,0.14094723761081696,0.164949432015419,0.06938079744577408,0.0961015373468399,0.2469320297241211,0.019096707925200462,0.02904338389635086,0.28214791417121887,0.02457173727452755,0.11211574077606201,0.2503632605075836,0.07929492741823196,0.029231691733002663,0.19174927473068237,0.0867728441953659,0.19158005714416504,0.13926485180854797,0.2533082067966461,0.24700891971588135,0.1891491711139679,0.18067772686481476,0.11062243580818176,0.19179630279541016,0.15137991309165955,0.10291151702404022,0.15088243782520294,0.10528920590877533,0.11526145786046982,0.1413499414920807,0.2230665236711502],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Mesure de performance pour le modele 3: avec 3 couches de GAT\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('42bcd5ad-7d80-4b01-b72a-fefaab173ae7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHIB0ayQLgYM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model4"
      ],
      "metadata": {
        "id": "wgm9diicQtxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv,SAGEConv,GATConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(24, hidden_channels,heads=3)\n",
        "        self.conv2 = GATConv(hidden_channels*3, hidden_channels,heads=3)\n",
        "        # self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
        "        # self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GATConv(hidden_channels*3, hidden_channels,heads = 3)\n",
        "        self.lin = Linear(hidden_channels*3, dataset.num_classes)\n",
        "\n",
        "    def forward(self,x,edge_index,batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x,batch)  \n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model4 = GCN(hidden_channels=64)\n",
        "print(model4)"
      ],
      "metadata": {
        "id": "NpL6vi2zQvlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043b62a8-829c-45c8-c88d-f47ecd44051f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GATConv(24, 64, heads=3)\n",
            "  (conv2): GATConv(192, 64, heads=3)\n",
            "  (conv3): GATConv(192, 64, heads=3)\n",
            "  (lin): Linear(in_features=192, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model4.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model4 = model4.to(device)\n",
        "dataset.data = dataset.data.to(device)\n",
        "# optimizer = torch.optim.Adam(list(model.parameters())+list(atom_encoder.parameters())+list(bond_encoder.parameters()), \n",
        "#                           lr=0.01)\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def train():\n",
        "  model4.train()\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    data.x = atom_encoder(data.x)\n",
        "    # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "  # Iterate in batches over the training dataset.\n",
        "    optimizer.zero_grad()\n",
        "    out = model4(data.x, data.edge_index, data.batch)  \n",
        "    loss = criterion(out, data.y.squeeze(1)) \n",
        "    loss.backward()  \n",
        "    optimizer.step()  \n",
        "    return float(loss)"
      ],
      "metadata": {
        "id": "jCvI-_uEQ2Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n",
        "def test(test_loader):\n",
        "    model4.eval()\n",
        "    y_label = []\n",
        "    y_predi = []\n",
        "    for data in test_loader:\n",
        "      data = data.to(device)\n",
        "      data.x = atom_encoder(data.x)\n",
        "      # data.edge_attr = bond_encoder(data.edge_attr)\n",
        "    # Iterate in batches over the training dataset.\n",
        "      out = model4(data.x,data.edge_index,data.batch)  # Perform a single forward pass.\n",
        "      y_predi_loader = out[:,1]\n",
        "      y_label.append(data.y.view(y_predi_loader.shape).detach().cpu())\n",
        "      y_predi.append(y_predi_loader.detach().cpu())\n",
        "    \n",
        "    y_label = torch.cat(y_label, dim = 0).numpy()\n",
        "    y_predi = torch.cat(y_predi, dim = 0).numpy()\n",
        "    # # y_label = np.array(y_label)\n",
        "    # # y_predi = np.array(y_label)\n",
        "    # evaluator = Evaluator(name='ogbg-molhiv')\n",
        "    # y_label = torch.tensor(y_label)\n",
        "    # y_predi = torch.tensor(y_predi)\n",
        "\n",
        "    precision = evaluator.eval({'y_true': y_label.reshape(len(y_label),1),\n",
        "                                'y_pred': y_predi.reshape(len(y_label),1),\n",
        "                               })\n",
        "    \n",
        "    return precision['rocauc']"
      ],
      "metadata": {
        "id": "nZMYs0_fROjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses4 = []\n",
        "train_acc_list4 = []\n",
        "test_acc_list4 = []\n",
        "\n",
        "for epoch in range(1, 150):\n",
        "    loss = train()\n",
        "    losses4.append(loss)\n",
        "    train_acc = test(train_loader)\n",
        "    train_acc_list4.append(train_acc)\n",
        "    test_acc = test(test_loader)\n",
        "    test_acc_list4.append(test_acc)\n",
        "    # if epoch % 10==0:\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "5ZvS-SwpRgdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de55b85-c906-4cc6-fc64-96690835d666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 0.7237, Train Acc: 0.5293, Test Acc: 0.5678\n",
            "Epoch: 002, Loss: 0.1561, Train Acc: 0.5103, Test Acc: 0.4867\n",
            "Epoch: 003, Loss: 0.3063, Train Acc: 0.5098, Test Acc: 0.4807\n",
            "Epoch: 004, Loss: 0.0123, Train Acc: 0.5059, Test Acc: 0.4767\n",
            "Epoch: 005, Loss: 0.0867, Train Acc: 0.5066, Test Acc: 0.4714\n",
            "Epoch: 006, Loss: 0.1161, Train Acc: 0.5088, Test Acc: 0.4667\n",
            "Epoch: 007, Loss: 0.1459, Train Acc: 0.5163, Test Acc: 0.4702\n",
            "Epoch: 008, Loss: 0.1049, Train Acc: 0.5271, Test Acc: 0.4785\n",
            "Epoch: 009, Loss: 0.3281, Train Acc: 0.5570, Test Acc: 0.5168\n",
            "Epoch: 010, Loss: 0.1783, Train Acc: 0.5756, Test Acc: 0.6061\n",
            "Epoch: 011, Loss: 0.2276, Train Acc: 0.5499, Test Acc: 0.5952\n",
            "Epoch: 012, Loss: 0.1154, Train Acc: 0.5480, Test Acc: 0.5876\n",
            "Epoch: 013, Loss: 0.1020, Train Acc: 0.5459, Test Acc: 0.5852\n",
            "Epoch: 014, Loss: 0.1690, Train Acc: 0.5420, Test Acc: 0.5819\n",
            "Epoch: 015, Loss: 0.0877, Train Acc: 0.5417, Test Acc: 0.5810\n",
            "Epoch: 016, Loss: 0.2284, Train Acc: 0.5413, Test Acc: 0.5841\n",
            "Epoch: 017, Loss: 0.0770, Train Acc: 0.5422, Test Acc: 0.5887\n",
            "Epoch: 018, Loss: 0.2114, Train Acc: 0.5455, Test Acc: 0.5922\n",
            "Epoch: 019, Loss: 0.0689, Train Acc: 0.5486, Test Acc: 0.5965\n",
            "Epoch: 020, Loss: 0.1127, Train Acc: 0.5494, Test Acc: 0.5980\n",
            "Epoch: 021, Loss: 0.1114, Train Acc: 0.5543, Test Acc: 0.6022\n",
            "Epoch: 022, Loss: 0.1363, Train Acc: 0.5585, Test Acc: 0.6041\n",
            "Epoch: 023, Loss: 0.2558, Train Acc: 0.5651, Test Acc: 0.6045\n",
            "Epoch: 024, Loss: 0.1490, Train Acc: 0.5758, Test Acc: 0.5905\n",
            "Epoch: 025, Loss: 0.3530, Train Acc: 0.5781, Test Acc: 0.5609\n",
            "Epoch: 026, Loss: 0.0440, Train Acc: 0.5724, Test Acc: 0.5190\n",
            "Epoch: 027, Loss: 0.0986, Train Acc: 0.5592, Test Acc: 0.4897\n",
            "Epoch: 028, Loss: 0.3067, Train Acc: 0.5574, Test Acc: 0.4877\n",
            "Epoch: 029, Loss: 0.1121, Train Acc: 0.5586, Test Acc: 0.4892\n",
            "Epoch: 030, Loss: 0.1505, Train Acc: 0.5556, Test Acc: 0.4859\n",
            "Epoch: 031, Loss: 0.0432, Train Acc: 0.5538, Test Acc: 0.4841\n",
            "Epoch: 032, Loss: 0.0886, Train Acc: 0.5552, Test Acc: 0.4854\n",
            "Epoch: 033, Loss: 0.1448, Train Acc: 0.5631, Test Acc: 0.4944\n",
            "Epoch: 034, Loss: 0.1234, Train Acc: 0.5711, Test Acc: 0.5048\n",
            "Epoch: 035, Loss: 0.0924, Train Acc: 0.5813, Test Acc: 0.5298\n",
            "Epoch: 036, Loss: 0.0773, Train Acc: 0.5869, Test Acc: 0.5686\n",
            "Epoch: 037, Loss: 0.1562, Train Acc: 0.5828, Test Acc: 0.6075\n",
            "Epoch: 038, Loss: 0.0995, Train Acc: 0.5724, Test Acc: 0.6170\n",
            "Epoch: 039, Loss: 0.1001, Train Acc: 0.5662, Test Acc: 0.6156\n",
            "Epoch: 040, Loss: 0.1566, Train Acc: 0.5617, Test Acc: 0.6131\n",
            "Epoch: 041, Loss: 0.0718, Train Acc: 0.5587, Test Acc: 0.6099\n",
            "Epoch: 042, Loss: 0.0857, Train Acc: 0.5568, Test Acc: 0.6067\n",
            "Epoch: 043, Loss: 0.1059, Train Acc: 0.5534, Test Acc: 0.6016\n",
            "Epoch: 044, Loss: 0.2254, Train Acc: 0.5511, Test Acc: 0.5998\n",
            "Epoch: 045, Loss: 0.1878, Train Acc: 0.5515, Test Acc: 0.6012\n",
            "Epoch: 046, Loss: 0.2458, Train Acc: 0.5528, Test Acc: 0.6045\n",
            "Epoch: 047, Loss: 0.1884, Train Acc: 0.5585, Test Acc: 0.6110\n",
            "Epoch: 048, Loss: 0.0726, Train Acc: 0.5644, Test Acc: 0.6171\n",
            "Epoch: 049, Loss: 0.0541, Train Acc: 0.5696, Test Acc: 0.6224\n",
            "Epoch: 050, Loss: 0.2495, Train Acc: 0.5792, Test Acc: 0.6317\n",
            "Epoch: 051, Loss: 0.1416, Train Acc: 0.5916, Test Acc: 0.6403\n",
            "Epoch: 052, Loss: 0.1603, Train Acc: 0.6057, Test Acc: 0.6449\n",
            "Epoch: 053, Loss: 0.2809, Train Acc: 0.6052, Test Acc: 0.6286\n",
            "Epoch: 054, Loss: 0.1348, Train Acc: 0.5824, Test Acc: 0.5761\n",
            "Epoch: 055, Loss: 0.0886, Train Acc: 0.5594, Test Acc: 0.5197\n",
            "Epoch: 056, Loss: 0.0898, Train Acc: 0.5472, Test Acc: 0.5009\n",
            "Epoch: 057, Loss: 0.1918, Train Acc: 0.5413, Test Acc: 0.4905\n",
            "Epoch: 058, Loss: 0.1222, Train Acc: 0.5391, Test Acc: 0.4873\n",
            "Epoch: 059, Loss: 0.0879, Train Acc: 0.5383, Test Acc: 0.4864\n",
            "Epoch: 060, Loss: 0.0276, Train Acc: 0.5374, Test Acc: 0.4881\n",
            "Epoch: 061, Loss: 0.0813, Train Acc: 0.5389, Test Acc: 0.4928\n",
            "Epoch: 062, Loss: 0.0836, Train Acc: 0.5442, Test Acc: 0.4998\n",
            "Epoch: 063, Loss: 0.0942, Train Acc: 0.5551, Test Acc: 0.5167\n",
            "Epoch: 064, Loss: 0.0070, Train Acc: 0.5677, Test Acc: 0.5405\n",
            "Epoch: 065, Loss: 0.2271, Train Acc: 0.5638, Test Acc: 0.5340\n",
            "Epoch: 066, Loss: 0.1204, Train Acc: 0.5623, Test Acc: 0.5304\n",
            "Epoch: 067, Loss: 0.2760, Train Acc: 0.5684, Test Acc: 0.5329\n",
            "Epoch: 068, Loss: 0.1287, Train Acc: 0.5769, Test Acc: 0.5459\n",
            "Epoch: 069, Loss: 0.1324, Train Acc: 0.5851, Test Acc: 0.5660\n",
            "Epoch: 070, Loss: 0.1518, Train Acc: 0.5924, Test Acc: 0.5879\n",
            "Epoch: 071, Loss: 0.2240, Train Acc: 0.5937, Test Acc: 0.6059\n",
            "Epoch: 072, Loss: 0.1196, Train Acc: 0.5863, Test Acc: 0.6077\n",
            "Epoch: 073, Loss: 0.0721, Train Acc: 0.5706, Test Acc: 0.5995\n",
            "Epoch: 074, Loss: 0.2421, Train Acc: 0.5653, Test Acc: 0.5992\n",
            "Epoch: 075, Loss: 0.1968, Train Acc: 0.5587, Test Acc: 0.5970\n",
            "Epoch: 076, Loss: 0.1998, Train Acc: 0.5566, Test Acc: 0.5961\n",
            "Epoch: 077, Loss: 0.3772, Train Acc: 0.5584, Test Acc: 0.5998\n",
            "Epoch: 078, Loss: 0.0777, Train Acc: 0.5577, Test Acc: 0.6011\n",
            "Epoch: 079, Loss: 0.2256, Train Acc: 0.5568, Test Acc: 0.6031\n",
            "Epoch: 080, Loss: 0.1567, Train Acc: 0.5555, Test Acc: 0.6042\n",
            "Epoch: 081, Loss: 0.1230, Train Acc: 0.5540, Test Acc: 0.6044\n",
            "Epoch: 082, Loss: 0.3372, Train Acc: 0.5528, Test Acc: 0.6039\n",
            "Epoch: 083, Loss: 0.1599, Train Acc: 0.5516, Test Acc: 0.6030\n",
            "Epoch: 084, Loss: 0.1470, Train Acc: 0.5514, Test Acc: 0.6029\n",
            "Epoch: 085, Loss: 0.2219, Train Acc: 0.5499, Test Acc: 0.6022\n",
            "Epoch: 086, Loss: 0.2942, Train Acc: 0.5505, Test Acc: 0.6038\n",
            "Epoch: 087, Loss: 0.0509, Train Acc: 0.5522, Test Acc: 0.6054\n",
            "Epoch: 088, Loss: 0.0402, Train Acc: 0.5553, Test Acc: 0.6087\n",
            "Epoch: 089, Loss: 0.2928, Train Acc: 0.5578, Test Acc: 0.6127\n",
            "Epoch: 090, Loss: 0.0287, Train Acc: 0.5615, Test Acc: 0.6181\n",
            "Epoch: 091, Loss: 0.0858, Train Acc: 0.5657, Test Acc: 0.6247\n",
            "Epoch: 092, Loss: 0.3292, Train Acc: 0.5697, Test Acc: 0.6321\n",
            "Epoch: 093, Loss: 0.0183, Train Acc: 0.5738, Test Acc: 0.6382\n",
            "Epoch: 094, Loss: 0.0787, Train Acc: 0.5765, Test Acc: 0.6433\n",
            "Epoch: 095, Loss: 0.1630, Train Acc: 0.5775, Test Acc: 0.6460\n",
            "Epoch: 096, Loss: 0.2296, Train Acc: 0.5778, Test Acc: 0.6470\n",
            "Epoch: 097, Loss: 0.1416, Train Acc: 0.5780, Test Acc: 0.6475\n",
            "Epoch: 098, Loss: 0.0431, Train Acc: 0.5782, Test Acc: 0.6480\n",
            "Epoch: 099, Loss: 0.0415, Train Acc: 0.5791, Test Acc: 0.6490\n",
            "Epoch: 100, Loss: 0.1860, Train Acc: 0.5780, Test Acc: 0.6478\n",
            "Epoch: 101, Loss: 0.2407, Train Acc: 0.5794, Test Acc: 0.6477\n",
            "Epoch: 102, Loss: 0.0404, Train Acc: 0.5815, Test Acc: 0.6474\n",
            "Epoch: 103, Loss: 0.0809, Train Acc: 0.5847, Test Acc: 0.6463\n",
            "Epoch: 104, Loss: 0.0819, Train Acc: 0.5897, Test Acc: 0.6403\n",
            "Epoch: 105, Loss: 0.1929, Train Acc: 0.5886, Test Acc: 0.6412\n",
            "Epoch: 106, Loss: 0.2191, Train Acc: 0.5887, Test Acc: 0.6419\n",
            "Epoch: 107, Loss: 0.3046, Train Acc: 0.5908, Test Acc: 0.6404\n",
            "Epoch: 108, Loss: 0.1389, Train Acc: 0.5933, Test Acc: 0.6382\n",
            "Epoch: 109, Loss: 0.0908, Train Acc: 0.5962, Test Acc: 0.6331\n",
            "Epoch: 110, Loss: 0.2921, Train Acc: 0.5980, Test Acc: 0.6264\n",
            "Epoch: 111, Loss: 0.1530, Train Acc: 0.6007, Test Acc: 0.6052\n",
            "Epoch: 112, Loss: 0.1037, Train Acc: 0.6008, Test Acc: 0.5813\n",
            "Epoch: 113, Loss: 0.1220, Train Acc: 0.5987, Test Acc: 0.5540\n",
            "Epoch: 114, Loss: 0.1465, Train Acc: 0.5919, Test Acc: 0.5172\n",
            "Epoch: 115, Loss: 0.1256, Train Acc: 0.5820, Test Acc: 0.4980\n",
            "Epoch: 116, Loss: 0.1831, Train Acc: 0.5668, Test Acc: 0.4856\n",
            "Epoch: 117, Loss: 0.0279, Train Acc: 0.5504, Test Acc: 0.4765\n",
            "Epoch: 118, Loss: 0.0891, Train Acc: 0.5433, Test Acc: 0.4744\n",
            "Epoch: 119, Loss: 0.1542, Train Acc: 0.5376, Test Acc: 0.4735\n",
            "Epoch: 120, Loss: 0.3092, Train Acc: 0.5425, Test Acc: 0.4765\n",
            "Epoch: 121, Loss: 0.2283, Train Acc: 0.5524, Test Acc: 0.4844\n",
            "Epoch: 122, Loss: 0.1141, Train Acc: 0.5609, Test Acc: 0.4938\n",
            "Epoch: 123, Loss: 0.2303, Train Acc: 0.5701, Test Acc: 0.5072\n",
            "Epoch: 124, Loss: 0.1744, Train Acc: 0.5766, Test Acc: 0.5247\n",
            "Epoch: 125, Loss: 0.0602, Train Acc: 0.5821, Test Acc: 0.5400\n",
            "Epoch: 126, Loss: 0.2022, Train Acc: 0.5898, Test Acc: 0.5682\n",
            "Epoch: 127, Loss: 0.2589, Train Acc: 0.5940, Test Acc: 0.5981\n",
            "Epoch: 128, Loss: 0.1061, Train Acc: 0.5967, Test Acc: 0.6165\n",
            "Epoch: 129, Loss: 0.0644, Train Acc: 0.5980, Test Acc: 0.6311\n",
            "Epoch: 130, Loss: 0.2007, Train Acc: 0.5964, Test Acc: 0.6438\n",
            "Epoch: 131, Loss: 0.1924, Train Acc: 0.5948, Test Acc: 0.6494\n",
            "Epoch: 132, Loss: 0.2236, Train Acc: 0.5944, Test Acc: 0.6538\n",
            "Epoch: 133, Loss: 0.0873, Train Acc: 0.5952, Test Acc: 0.6555\n",
            "Epoch: 134, Loss: 0.1991, Train Acc: 0.5970, Test Acc: 0.6514\n",
            "Epoch: 135, Loss: 0.0280, Train Acc: 0.5970, Test Acc: 0.6449\n",
            "Epoch: 136, Loss: 0.3207, Train Acc: 0.5957, Test Acc: 0.6419\n",
            "Epoch: 137, Loss: 0.1437, Train Acc: 0.5945, Test Acc: 0.6413\n",
            "Epoch: 138, Loss: 0.1426, Train Acc: 0.5917, Test Acc: 0.6410\n",
            "Epoch: 139, Loss: 0.0865, Train Acc: 0.5890, Test Acc: 0.6433\n",
            "Epoch: 140, Loss: 0.1294, Train Acc: 0.5880, Test Acc: 0.6415\n",
            "Epoch: 141, Loss: 0.2109, Train Acc: 0.5855, Test Acc: 0.6409\n",
            "Epoch: 142, Loss: 0.1933, Train Acc: 0.5857, Test Acc: 0.6358\n",
            "Epoch: 143, Loss: 0.2539, Train Acc: 0.5872, Test Acc: 0.6331\n",
            "Epoch: 144, Loss: 0.1542, Train Acc: 0.5882, Test Acc: 0.6316\n",
            "Epoch: 145, Loss: 0.1308, Train Acc: 0.5886, Test Acc: 0.6285\n",
            "Epoch: 146, Loss: 0.1076, Train Acc: 0.5908, Test Acc: 0.6279\n",
            "Epoch: 147, Loss: 0.0978, Train Acc: 0.5928, Test Acc: 0.6253\n",
            "Epoch: 148, Loss: 0.1909, Train Acc: 0.5986, Test Acc: 0.6286\n",
            "Epoch: 149, Loss: 0.1796, Train Acc: 0.6049, Test Acc: 0.6271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = pd.DataFrame(list(zip(train_acc_list4,test_acc_list4, losses4)), columns =['train_acc4', 'test_acc4','loss4'])"
      ],
      "metadata": {
        "id": "BqFpBUrzSAOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses_float = [loss for loss in losses4] \n",
        "loss_indices = [i for i,l in enumerate(losses_float)] \n",
        "fig = px.line(df4, x=loss_indices, y=[\"train_acc4\", \"test_acc4\", \"loss4\"], title=\"Mesure de performance pour le modele 3: avec 3 couches de GAT et self attention multi tete\",\n",
        "            labels={\"train_acc4\": \"train_acc4\", \"loss4\": \"loss4\",\"test_acc4\":\"test_acc4\"})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "IKkC3VLiSB2A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "a6194ca8-a17d-48f5-f378-c0b66d97d888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"c01f248f-dc34-4b0d-b23f-e3f86dd28643\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c01f248f-dc34-4b0d-b23f-e3f86dd28643\")) {                    Plotly.newPlot(                        \"c01f248f-dc34-4b0d-b23f-e3f86dd28643\",                        [{\"hovertemplate\":\"variable=train_acc4<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"train_acc4\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train_acc4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\"xaxis\":\"x\",\"y\":[0.5293339629520121,0.5102527646971741,0.509783382844381,0.5059121967978026,0.5066143537065416,0.5087533878228248,0.5163136176637155,0.5270992762802578,0.5570481503481834,0.5756354384823866,0.5498881131656874,0.5480405732920021,0.545898617313092,0.5419977000327658,0.5417092814350385,0.5412892764976033,0.542188518166604,0.5455372803015608,0.5485795672403633,0.5493624726421389,0.554343428249101,0.5584534124894966,0.5651380831268893,0.5757699851302822,0.5780576497849303,0.5724426884334839,0.5592211703402853,0.557443242562168,0.558562597882398,0.5556059292076769,0.5537902991495227,0.5551989060546325,0.5630969826693563,0.5711049495122642,0.5812960731601595,0.586920735608135,0.5827991151779793,0.5724133929160927,0.566156903305416,0.5617041512593945,0.5587279266093722,0.5567583348950774,0.5534029857540231,0.5511367224615985,0.5514969573670512,0.5527658402887333,0.5584705848400233,0.5644171135236925,0.5696226937276939,0.5791686111577015,0.5916233325391336,0.6057204739117652,0.6052491569657409,0.5823692297313978,0.559375695864652,0.5472455575385491,0.5413137406895103,0.5391116686685697,0.5382951234010236,0.5373750954987733,0.5389001411926039,0.5442212631222388,0.5550591051800831,0.5676581127515006,0.5638154276807218,0.5622973662637845,0.5683723671967302,0.5769132407741931,0.5850617261421202,0.5924177075332385,0.5936828227899543,0.586331070410533,0.5706493824310143,0.5653272224712356,0.5587290671610117,0.5565966072356391,0.558383018667524,0.5576729163428696,0.5568498353299737,0.5555046123395693,0.5539941887740603,0.5528155375837652,0.5516124990926847,0.5513579382189063,0.5499051189187837,0.5505177361162314,0.5522486065278307,0.5553121282314263,0.557758214227277,0.5614801161609555,0.5656672990875997,0.5697378509977187,0.5737573984637359,0.5764990795620117,0.5774699324957464,0.57781114966375,0.5779717752171097,0.5782046527945515,0.5791398666933496,0.5780069785356896,0.5793535214903509,0.5815295786817622,0.5847478950286507,0.5896618656533715,0.5886374836837039,0.5887392747137292,0.59080879925594,0.5932991104619906,0.5961823737458033,0.598030746606641,0.6007352764779191,0.6007696724397205,0.5987272263875567,0.5918615412343506,0.5820404817403064,0.5668266762367067,0.5503504466656524,0.5432919442094424,0.5376315786505956,0.5424586110469782,0.552401619860136,0.5608823645803815,0.570117846921464,0.5766396749781526,0.582128650226593,0.5897799319708363,0.593978238479762,0.5967261605740876,0.5979745904573812,0.5964386903001953,0.5948346133483807,0.594423476520322,0.5952086630253766,0.5969927805387956,0.5969715201436285,0.595687617822829,0.5944739939873193,0.5917024406882392,0.5889572225983621,0.5879700687468141,0.5855015946193438,0.5857389703274085,0.5872387060269927,0.588213531668685,0.5885871071837632,0.5908486160418255,0.5927795443370611,0.598565293685127,0.6049162312237006],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=test_acc4<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"test_acc4\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test_acc4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\"xaxis\":\"x\",\"y\":[0.5677794086405685,0.4866664091620155,0.480650456748875,0.4767415361439966,0.4713609764576373,0.4667355491608567,0.4702350373703625,0.4785067305278202,0.516825353907955,0.6061163792270998,0.595206550918326,0.5875934259062555,0.5851909075107669,0.5819463489059272,0.5809845690337782,0.5840755904903533,0.5886672203016666,0.592184090075127,0.5965333436335194,0.5980165704243033,0.602153382645474,0.604075011104888,0.6045288630525889,0.5905212537901465,0.5608914811023775,0.5190308812452924,0.48966472894416657,0.487689990150447,0.4892408119121652,0.4858977577782498,0.4841422198188455,0.4854284555514784,0.4944050676915352,0.5047886208694645,0.5298460765947586,0.5685528882365438,0.6074731068579926,0.6170049634021515,0.6156472701288167,0.6130767299484347,0.6099345294424381,0.6067005929044593,0.6015643407559049,0.5998059058691747,0.6012109156221634,0.6044651306514224,0.6110498464628517,0.617068695803318,0.6224328395681646,0.6316672782402132,0.640284671391877,0.6448791981305162,0.6285801193534059,0.576122559338728,0.5196991058150988,0.5009308793140076,0.4904613839587477,0.48726124490623607,0.48643079240618786,0.48810328511558737,0.492750922188532,0.4998435659244095,0.5166534695532938,0.5404777998802603,0.533984820100813,0.5303984240715348,0.5328994379960988,0.5459027791189478,0.5659842793410457,0.5879391259004616,0.6058575870526662,0.607653681994631,0.5995268352034608,0.5991792039243709,0.5969698140172657,0.5961239112381468,0.5998049402267328,0.601100832383785,0.603080399389714,0.6041850943432665,0.6044168485293266,0.6039398211630198,0.6029567971571487,0.6029258965990072,0.602245118677456,0.6038220127851059,0.6053631781224049,0.6086782286255046,0.6127039919658548,0.6181309024894261,0.624699202379343,0.6320882983448889,0.6382220591359431,0.6432511249734449,0.6460476254852354,0.6470016802178489,0.6474787075841557,0.648007879642326,0.6489551748778462,0.6478089572992912,0.6476544545085845,0.647358967921358,0.6462677919619922,0.6402730836825741,0.6412290697000714,0.6418741188512718,0.6403754417814171,0.6381853647231504,0.6330771162054115,0.6264296336352576,0.6052279881805366,0.5813148187489137,0.553971687363603,0.5172425114428629,0.4980474709824446,0.48563317174916476,0.4764808126846791,0.474387299870604,0.47353946580660117,0.4765194383823558,0.48444350026072347,0.49381988837173374,0.5072075551864655,0.5247243090828329,0.5400355356418626,0.5682091195272215,0.5981044438865176,0.6165289016782866,0.6311419687518105,0.6438305104385948,0.6494408930261303,0.6537582803839395,0.6555398906892755,0.6513837656192665,0.6449197551130766,0.6418963286274358,0.6412850769617026,0.6410127657930822,0.6433119604472856,0.641536143996601,0.6408601942872594,0.6358485100137121,0.6330809787751792,0.6315803704204408,0.6284652079028177,0.627918654280693,0.6253442515305433,0.6285907414202669,0.6271210336236699],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"variable=loss4<br>x=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"loss4\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"loss4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148],\"xaxis\":\"x\",\"y\":[0.7237395644187927,0.1561499536037445,0.30631694197654724,0.012301919050514698,0.08668351173400879,0.11608709394931793,0.14590628445148468,0.10485371947288513,0.32808196544647217,0.17828668653964996,0.22764809429645538,0.11537517607212067,0.1020437628030777,0.16899769008159637,0.08770886808633804,0.2283988744020462,0.07698097825050354,0.21139535307884216,0.06888685375452042,0.11267216503620148,0.11141805350780487,0.13633930683135986,0.2558130621910095,0.14899155497550964,0.3529917299747467,0.043987225741147995,0.09860113263130188,0.3066660761833191,0.1121373325586319,0.15049433708190918,0.04315272718667984,0.08862493187189102,0.14475324749946594,0.12340307980775833,0.09238758683204651,0.07726993411779404,0.15622031688690186,0.09947105497121811,0.10008633136749268,0.15659885108470917,0.07184891402721405,0.08572075515985489,0.10588341951370239,0.2254229187965393,0.18780864775180817,0.24582596123218536,0.18838582932949066,0.07259069383144379,0.05414174869656563,0.24952180683612823,0.14158694446086884,0.1602962762117386,0.2808781564235687,0.13476479053497314,0.08864394575357437,0.08982624858617783,0.19178301095962524,0.12220483273267746,0.08786347508430481,0.027565203607082367,0.08126478642225266,0.08355686813592911,0.09418214857578278,0.007007397711277008,0.22711089253425598,0.12044577300548553,0.2760007977485657,0.12867380678653717,0.13244479894638062,0.15184156596660614,0.2239643782377243,0.11955765634775162,0.07206042110919952,0.24214409291744232,0.19678807258605957,0.1998441219329834,0.3771960437297821,0.07774822413921356,0.22556312382221222,0.156666100025177,0.12298338860273361,0.3372393846511841,0.1599476933479309,0.14697018265724182,0.22191481292247772,0.2941579222679138,0.050900086760520935,0.040158092975616455,0.29277369379997253,0.02870340086519718,0.08580607920885086,0.3291909694671631,0.018338212743401527,0.07871417701244354,0.16301345825195312,0.22964192926883698,0.1415819227695465,0.04313560947775841,0.041488420218229294,0.18604998290538788,0.24067534506320953,0.04042624682188034,0.08087200671434402,0.08186838775873184,0.19292591512203217,0.2191479355096817,0.30459949374198914,0.13885843753814697,0.09081588685512543,0.2921035885810852,0.1529529094696045,0.10368896275758743,0.12200252711772919,0.14645953476428986,0.12564803659915924,0.18312273919582367,0.02787497267127037,0.08913742005825043,0.15420706570148468,0.3092081546783447,0.22831782698631287,0.11405152082443237,0.23028351366519928,0.1743951141834259,0.06015986576676369,0.20220312476158142,0.2588895857334137,0.10608529299497604,0.06439730525016785,0.2007322609424591,0.19238902628421783,0.22361750900745392,0.08728628605604172,0.19909855723381042,0.027972202748060226,0.3207406997680664,0.14373257756233215,0.14262494444847107,0.08647233992815018,0.12944722175598145,0.2108563780784607,0.19331631064414978,0.2539154589176178,0.15422633290290833,0.1307695358991623,0.10760831832885742,0.09776902198791504,0.19089674949645996,0.1795840710401535],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Mesure de performance pour le modele 3: avec 3 couches de GAT et self attention multi tete\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c01f248f-dc34-4b0d-b23f-e3f86dd28643');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "495mtycCWzBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}